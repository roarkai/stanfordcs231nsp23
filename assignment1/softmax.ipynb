{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149d5b7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:02:26.525294Z",
     "start_time": "2024-07-24T09:02:26.381756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/roark/Documents/coursesDL/cs231n_2023/cs231nsp23HW/assignment1/cs231n/datasets\n",
      "/home/roark/Documents/coursesDL/cs231n_2023/cs231nsp23HW/assignment1\n"
     ]
    }
   ],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = '/home/roark/Documents/coursesDL/cs231n_2023/cs231nsp23HW/assignment1'\n",
    "\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "sys.path.append(FOLDERNAME)\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# !bash get_datasets.sh\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "%cd $FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd $FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfc27e",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6cf2d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:02:33.044454Z",
     "start_time": "2024-07-24T09:02:32.661664Z"
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9bbbbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:02:37.548327Z",
     "start_time": "2024-07-24T09:02:35.864344Z"
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3fb04",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f2e5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:02:41.666873Z",
     "start_time": "2024-07-24T09:02:41.567454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.382442\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebc638",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "$$Loss = -\\frac{1}{N}∑_{i=1}^Nlog\\frac{e^{S_{yi}}}{∑_je^S_j}$$\n",
    "当w初始化为均值为0的极小值时，scores也大约为0附件的很小的值，因此$$e^{s_j}≈1$$\n",
    "$$-\\frac{1}{N}∑_{i=1}^Nlog\\frac{e^{S_{yi}}}{∑_je^S_j}≈-\\frac{1}{N}*N*log\\frac{1}{C}≈-log\\frac{1}{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cb3eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:01:49.828495Z",
     "start_time": "2024-07-24T13:01:46.861274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.476615 analytic: 4.476615, relative error: 3.310948e-08\n",
      "numerical: 0.356372 analytic: 0.356372, relative error: 1.119414e-07\n",
      "numerical: -0.755814 analytic: -0.755814, relative error: 2.471785e-08\n",
      "numerical: 0.308663 analytic: 0.308663, relative error: 3.476294e-07\n",
      "numerical: 4.691961 analytic: 4.691961, relative error: 3.167460e-08\n",
      "numerical: -0.232183 analytic: -0.232183, relative error: 9.987430e-08\n",
      "numerical: -0.361611 analytic: -0.361611, relative error: 3.067619e-08\n",
      "numerical: 2.376714 analytic: 2.376714, relative error: 5.254403e-10\n",
      "numerical: 0.996779 analytic: 0.996778, relative error: 1.205909e-07\n",
      "numerical: 2.796696 analytic: 2.796696, relative error: 3.296308e-08\n",
      "numerical: -0.073047 analytic: -0.073047, relative error: 6.829955e-09\n",
      "numerical: 0.279318 analytic: 0.279318, relative error: 9.759090e-08\n",
      "numerical: 5.603353 analytic: 5.603352, relative error: 2.076887e-08\n",
      "numerical: 2.879929 analytic: 2.879929, relative error: 1.372671e-08\n",
      "numerical: -0.383268 analytic: -0.383269, relative error: 6.252573e-08\n",
      "numerical: 2.660182 analytic: 2.660182, relative error: 9.768733e-09\n",
      "numerical: -3.111002 analytic: -3.111002, relative error: 8.748379e-09\n",
      "numerical: -5.401432 analytic: -5.401432, relative error: 7.348959e-09\n",
      "numerical: 1.503746 analytic: 1.503746, relative error: 5.715708e-09\n",
      "numerical: -3.424785 analytic: -3.424785, relative error: 1.261157e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4a81d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:08:46.930707Z",
     "start_time": "2024-07-24T13:08:46.805352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.382442e+00 computed in 0.110460s\n",
      "vectorized loss: 2.382442e+00 computed in 0.001796s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3453536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:21:08.232429Z",
     "start_time": "2024-07-24T13:12:44.753252Z"
    },
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 6.006476\n",
      "iteration 100 / 2000: loss 3.367348\n",
      "iteration 200 / 2000: loss 2.987008\n",
      "iteration 300 / 2000: loss 2.985845\n",
      "iteration 400 / 2000: loss 3.017161\n",
      "iteration 500 / 2000: loss 2.951382\n",
      "iteration 600 / 2000: loss 2.885876\n",
      "iteration 700 / 2000: loss 2.736034\n",
      "iteration 800 / 2000: loss 2.528246\n",
      "iteration 900 / 2000: loss 2.507417\n",
      "iteration 1000 / 2000: loss 2.728991\n",
      "iteration 1100 / 2000: loss 2.579596\n",
      "iteration 1200 / 2000: loss 2.589637\n",
      "iteration 1300 / 2000: loss 2.543153\n",
      "iteration 1400 / 2000: loss 2.513352\n",
      "iteration 1500 / 2000: loss 2.443709\n",
      "iteration 1600 / 2000: loss 2.420289\n",
      "iteration 1700 / 2000: loss 2.608852\n",
      "iteration 1800 / 2000: loss 2.581663\n",
      "iteration 1900 / 2000: loss 2.342864\n",
      "iteration 0 / 2000: loss 13.537786\n",
      "iteration 100 / 2000: loss 10.159590\n",
      "iteration 200 / 2000: loss 9.540649\n",
      "iteration 300 / 2000: loss 9.352724\n",
      "iteration 400 / 2000: loss 8.781295\n",
      "iteration 500 / 2000: loss 8.229303\n",
      "iteration 600 / 2000: loss 8.247981\n",
      "iteration 700 / 2000: loss 8.058636\n",
      "iteration 800 / 2000: loss 7.829901\n",
      "iteration 900 / 2000: loss 7.416044\n",
      "iteration 1000 / 2000: loss 7.378551\n",
      "iteration 1100 / 2000: loss 7.095888\n",
      "iteration 1200 / 2000: loss 6.914858\n",
      "iteration 1300 / 2000: loss 6.957996\n",
      "iteration 1400 / 2000: loss 6.583189\n",
      "iteration 1500 / 2000: loss 6.386236\n",
      "iteration 1600 / 2000: loss 6.332046\n",
      "iteration 1700 / 2000: loss 6.228631\n",
      "iteration 1800 / 2000: loss 6.118346\n",
      "iteration 1900 / 2000: loss 5.959847\n",
      "iteration 0 / 2000: loss 20.296481\n",
      "iteration 100 / 2000: loss 16.265026\n",
      "iteration 200 / 2000: loss 15.023307\n",
      "iteration 300 / 2000: loss 14.242134\n",
      "iteration 400 / 2000: loss 13.363335\n",
      "iteration 500 / 2000: loss 12.650264\n",
      "iteration 600 / 2000: loss 12.253974\n",
      "iteration 700 / 2000: loss 11.274039\n",
      "iteration 800 / 2000: loss 10.873787\n",
      "iteration 900 / 2000: loss 10.410645\n",
      "iteration 1000 / 2000: loss 9.947959\n",
      "iteration 1100 / 2000: loss 9.485212\n",
      "iteration 1200 / 2000: loss 9.072787\n",
      "iteration 1300 / 2000: loss 8.672192\n",
      "iteration 1400 / 2000: loss 8.061938\n",
      "iteration 1500 / 2000: loss 7.908766\n",
      "iteration 1600 / 2000: loss 7.533418\n",
      "iteration 1700 / 2000: loss 7.210979\n",
      "iteration 1800 / 2000: loss 6.735679\n",
      "iteration 1900 / 2000: loss 6.729862\n",
      "iteration 0 / 2000: loss 27.059313\n",
      "iteration 100 / 2000: loss 22.294215\n",
      "iteration 200 / 2000: loss 20.337657\n",
      "iteration 300 / 2000: loss 18.925895\n",
      "iteration 400 / 2000: loss 17.455275\n",
      "iteration 500 / 2000: loss 16.196621\n",
      "iteration 600 / 2000: loss 14.907030\n",
      "iteration 700 / 2000: loss 13.816059\n",
      "iteration 800 / 2000: loss 12.550761\n",
      "iteration 900 / 2000: loss 11.972586\n",
      "iteration 1000 / 2000: loss 10.927876\n",
      "iteration 1100 / 2000: loss 10.251890\n",
      "iteration 1200 / 2000: loss 9.671747\n",
      "iteration 1300 / 2000: loss 8.925791\n",
      "iteration 1400 / 2000: loss 8.534670\n",
      "iteration 1500 / 2000: loss 8.076417\n",
      "iteration 1600 / 2000: loss 7.368946\n",
      "iteration 1700 / 2000: loss 6.873625\n",
      "iteration 1800 / 2000: loss 6.510869\n",
      "iteration 1900 / 2000: loss 6.194836\n",
      "iteration 0 / 2000: loss 32.799697\n",
      "iteration 100 / 2000: loss 27.697689\n",
      "iteration 200 / 2000: loss 24.799309\n",
      "iteration 300 / 2000: loss 22.226898\n",
      "iteration 400 / 2000: loss 20.020145\n",
      "iteration 500 / 2000: loss 18.025516\n",
      "iteration 600 / 2000: loss 16.167890\n",
      "iteration 700 / 2000: loss 14.669223\n",
      "iteration 800 / 2000: loss 13.345012\n",
      "iteration 900 / 2000: loss 12.175207\n",
      "iteration 1000 / 2000: loss 11.043346\n",
      "iteration 1100 / 2000: loss 10.113855\n",
      "iteration 1200 / 2000: loss 9.174846\n",
      "iteration 1300 / 2000: loss 8.285148\n",
      "iteration 1400 / 2000: loss 7.771900\n",
      "iteration 1500 / 2000: loss 6.993263\n",
      "iteration 1600 / 2000: loss 6.702322\n",
      "iteration 1700 / 2000: loss 6.196903\n",
      "iteration 1800 / 2000: loss 5.634812\n",
      "iteration 1900 / 2000: loss 5.274544\n",
      "iteration 0 / 2000: loss 39.948260\n",
      "iteration 100 / 2000: loss 33.141944\n",
      "iteration 200 / 2000: loss 28.934424\n",
      "iteration 300 / 2000: loss 25.444855\n",
      "iteration 400 / 2000: loss 22.121024\n",
      "iteration 500 / 2000: loss 19.585907\n",
      "iteration 600 / 2000: loss 17.204138\n",
      "iteration 700 / 2000: loss 15.295847\n",
      "iteration 800 / 2000: loss 13.645389\n",
      "iteration 900 / 2000: loss 11.932385\n",
      "iteration 1000 / 2000: loss 10.660746\n",
      "iteration 1100 / 2000: loss 9.675912\n",
      "iteration 1200 / 2000: loss 8.618757\n",
      "iteration 1300 / 2000: loss 7.808349\n",
      "iteration 1400 / 2000: loss 7.076810\n",
      "iteration 1500 / 2000: loss 6.339500\n",
      "iteration 1600 / 2000: loss 5.684796\n",
      "iteration 1700 / 2000: loss 5.187755\n",
      "iteration 1800 / 2000: loss 4.704382\n",
      "iteration 1900 / 2000: loss 4.405364\n",
      "iteration 0 / 2000: loss 46.645068\n",
      "iteration 100 / 2000: loss 37.898389\n",
      "iteration 200 / 2000: loss 32.086948\n",
      "iteration 300 / 2000: loss 27.309109\n",
      "iteration 400 / 2000: loss 23.659878\n",
      "iteration 500 / 2000: loss 20.231961\n",
      "iteration 600 / 2000: loss 17.294479\n",
      "iteration 700 / 2000: loss 15.102661\n",
      "iteration 800 / 2000: loss 13.094681\n",
      "iteration 900 / 2000: loss 11.418552\n",
      "iteration 1000 / 2000: loss 9.900224\n",
      "iteration 1100 / 2000: loss 8.699970\n",
      "iteration 1200 / 2000: loss 7.650154\n",
      "iteration 1300 / 2000: loss 6.742749\n",
      "iteration 1400 / 2000: loss 6.052388\n",
      "iteration 1500 / 2000: loss 5.346408\n",
      "iteration 1600 / 2000: loss 4.847783\n",
      "iteration 1700 / 2000: loss 4.530797\n",
      "iteration 1800 / 2000: loss 4.202051\n",
      "iteration 1900 / 2000: loss 3.603701\n",
      "iteration 0 / 2000: loss 53.934653\n",
      "iteration 100 / 2000: loss 43.304560\n",
      "iteration 200 / 2000: loss 35.745277\n",
      "iteration 300 / 2000: loss 30.072760\n",
      "iteration 400 / 2000: loss 24.953975\n",
      "iteration 500 / 2000: loss 20.938025\n",
      "iteration 600 / 2000: loss 17.597615\n",
      "iteration 700 / 2000: loss 14.873257\n",
      "iteration 800 / 2000: loss 12.626149\n",
      "iteration 900 / 2000: loss 10.828888\n",
      "iteration 1000 / 2000: loss 9.139488\n",
      "iteration 1100 / 2000: loss 7.945741\n",
      "iteration 1200 / 2000: loss 6.891320\n",
      "iteration 1300 / 2000: loss 5.963775\n",
      "iteration 1400 / 2000: loss 5.333753\n",
      "iteration 1500 / 2000: loss 4.744139\n",
      "iteration 1600 / 2000: loss 4.142165\n",
      "iteration 1700 / 2000: loss 3.752267\n",
      "iteration 1800 / 2000: loss 3.449408\n",
      "iteration 1900 / 2000: loss 3.257494\n",
      "iteration 0 / 2000: loss 60.135033\n",
      "iteration 100 / 2000: loss 47.485128\n",
      "iteration 200 / 2000: loss 38.045250\n",
      "iteration 300 / 2000: loss 31.025301\n",
      "iteration 400 / 2000: loss 25.163492\n",
      "iteration 500 / 2000: loss 20.654852\n",
      "iteration 600 / 2000: loss 16.818645\n",
      "iteration 700 / 2000: loss 14.037851\n",
      "iteration 800 / 2000: loss 11.621978\n",
      "iteration 900 / 2000: loss 9.791003\n",
      "iteration 1000 / 2000: loss 8.225340\n",
      "iteration 1100 / 2000: loss 6.918976\n",
      "iteration 1200 / 2000: loss 5.993669\n",
      "iteration 1300 / 2000: loss 5.075105\n",
      "iteration 1400 / 2000: loss 4.550398\n",
      "iteration 1500 / 2000: loss 4.000311\n",
      "iteration 1600 / 2000: loss 3.546469\n",
      "iteration 1700 / 2000: loss 3.223550\n",
      "iteration 1800 / 2000: loss 3.019528\n",
      "iteration 1900 / 2000: loss 2.848148\n",
      "iteration 0 / 2000: loss 68.105612\n",
      "iteration 100 / 2000: loss 51.751718\n",
      "iteration 200 / 2000: loss 40.664183\n",
      "iteration 300 / 2000: loss 31.865676\n",
      "iteration 400 / 2000: loss 25.532920\n",
      "iteration 500 / 2000: loss 20.458591\n",
      "iteration 600 / 2000: loss 16.406171\n",
      "iteration 700 / 2000: loss 13.226813\n",
      "iteration 800 / 2000: loss 10.745367\n",
      "iteration 900 / 2000: loss 8.841935\n",
      "iteration 1000 / 2000: loss 7.324985\n",
      "iteration 1100 / 2000: loss 6.172622\n",
      "iteration 1200 / 2000: loss 5.329586\n",
      "iteration 1300 / 2000: loss 4.476312\n",
      "iteration 1400 / 2000: loss 3.929543\n",
      "iteration 1500 / 2000: loss 3.629851\n",
      "iteration 1600 / 2000: loss 3.117071\n",
      "iteration 1700 / 2000: loss 2.939778\n",
      "iteration 1800 / 2000: loss 2.655063\n",
      "iteration 1900 / 2000: loss 2.513333\n",
      "iteration 0 / 2000: loss 6.632262\n",
      "iteration 100 / 2000: loss 3.302291\n",
      "iteration 200 / 2000: loss 3.026203\n",
      "iteration 300 / 2000: loss 2.971841\n",
      "iteration 400 / 2000: loss 2.813945\n",
      "iteration 500 / 2000: loss 2.841061\n",
      "iteration 600 / 2000: loss 2.532936\n",
      "iteration 700 / 2000: loss 2.401761\n",
      "iteration 800 / 2000: loss 2.491436\n",
      "iteration 900 / 2000: loss 2.393857\n",
      "iteration 1000 / 2000: loss 2.596068\n",
      "iteration 1100 / 2000: loss 2.432470\n",
      "iteration 1200 / 2000: loss 2.281362\n",
      "iteration 1300 / 2000: loss 2.535683\n",
      "iteration 1400 / 2000: loss 2.374588\n",
      "iteration 1500 / 2000: loss 2.499783\n",
      "iteration 1600 / 2000: loss 2.260989\n",
      "iteration 1700 / 2000: loss 2.240441\n",
      "iteration 1800 / 2000: loss 2.374287\n",
      "iteration 1900 / 2000: loss 2.332957\n",
      "iteration 0 / 2000: loss 12.297097\n",
      "iteration 100 / 2000: loss 9.691961\n",
      "iteration 200 / 2000: loss 9.123262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: loss 8.549719\n",
      "iteration 400 / 2000: loss 8.179948\n",
      "iteration 500 / 2000: loss 8.021295\n",
      "iteration 600 / 2000: loss 7.419436\n",
      "iteration 700 / 2000: loss 7.160181\n",
      "iteration 800 / 2000: loss 6.958036\n",
      "iteration 900 / 2000: loss 6.450384\n",
      "iteration 1000 / 2000: loss 6.384537\n",
      "iteration 1100 / 2000: loss 6.091222\n",
      "iteration 1200 / 2000: loss 5.861746\n",
      "iteration 1300 / 2000: loss 5.793336\n",
      "iteration 1400 / 2000: loss 5.606516\n",
      "iteration 1500 / 2000: loss 5.404273\n",
      "iteration 1600 / 2000: loss 5.057147\n",
      "iteration 1700 / 2000: loss 4.893401\n",
      "iteration 1800 / 2000: loss 4.958209\n",
      "iteration 1900 / 2000: loss 4.700588\n",
      "iteration 0 / 2000: loss 19.541647\n",
      "iteration 100 / 2000: loss 15.677672\n",
      "iteration 200 / 2000: loss 14.360120\n",
      "iteration 300 / 2000: loss 12.816803\n",
      "iteration 400 / 2000: loss 11.923259\n",
      "iteration 500 / 2000: loss 10.903041\n",
      "iteration 600 / 2000: loss 10.010185\n",
      "iteration 700 / 2000: loss 9.454154\n",
      "iteration 800 / 2000: loss 8.672840\n",
      "iteration 900 / 2000: loss 8.057209\n",
      "iteration 1000 / 2000: loss 7.579213\n",
      "iteration 1100 / 2000: loss 6.977388\n",
      "iteration 1200 / 2000: loss 6.367831\n",
      "iteration 1300 / 2000: loss 6.146590\n",
      "iteration 1400 / 2000: loss 5.736034\n",
      "iteration 1500 / 2000: loss 5.365062\n",
      "iteration 1600 / 2000: loss 5.153672\n",
      "iteration 1700 / 2000: loss 4.896082\n",
      "iteration 1800 / 2000: loss 4.582373\n",
      "iteration 1900 / 2000: loss 4.243927\n",
      "iteration 0 / 2000: loss 26.500485\n",
      "iteration 100 / 2000: loss 21.229418\n",
      "iteration 200 / 2000: loss 18.278622\n",
      "iteration 300 / 2000: loss 16.147024\n",
      "iteration 400 / 2000: loss 14.314016\n",
      "iteration 500 / 2000: loss 12.615371\n",
      "iteration 600 / 2000: loss 11.213860\n",
      "iteration 700 / 2000: loss 9.895711\n",
      "iteration 800 / 2000: loss 9.081255\n",
      "iteration 900 / 2000: loss 7.906418\n",
      "iteration 1000 / 2000: loss 7.236586\n",
      "iteration 1100 / 2000: loss 6.412185\n",
      "iteration 1200 / 2000: loss 6.012240\n",
      "iteration 1300 / 2000: loss 5.508083\n",
      "iteration 1400 / 2000: loss 4.892176\n",
      "iteration 1500 / 2000: loss 4.567280\n",
      "iteration 1600 / 2000: loss 4.200037\n",
      "iteration 1700 / 2000: loss 3.918623\n",
      "iteration 1800 / 2000: loss 3.751143\n",
      "iteration 1900 / 2000: loss 3.391344\n",
      "iteration 0 / 2000: loss 33.520247\n",
      "iteration 100 / 2000: loss 25.648390\n",
      "iteration 200 / 2000: loss 21.594909\n",
      "iteration 300 / 2000: loss 18.199525\n",
      "iteration 400 / 2000: loss 15.559058\n",
      "iteration 500 / 2000: loss 13.200168\n",
      "iteration 600 / 2000: loss 11.438545\n",
      "iteration 700 / 2000: loss 9.819649\n",
      "iteration 800 / 2000: loss 8.508156\n",
      "iteration 900 / 2000: loss 7.332942\n",
      "iteration 1000 / 2000: loss 6.485557\n",
      "iteration 1100 / 2000: loss 5.724886\n",
      "iteration 1200 / 2000: loss 4.891475\n",
      "iteration 1300 / 2000: loss 4.443540\n",
      "iteration 1400 / 2000: loss 4.186372\n",
      "iteration 1500 / 2000: loss 3.670038\n",
      "iteration 1600 / 2000: loss 3.409501\n",
      "iteration 1700 / 2000: loss 3.129596\n",
      "iteration 1800 / 2000: loss 2.860677\n",
      "iteration 1900 / 2000: loss 2.731835\n",
      "iteration 0 / 2000: loss 39.306892\n",
      "iteration 100 / 2000: loss 30.184958\n",
      "iteration 200 / 2000: loss 24.217969\n",
      "iteration 300 / 2000: loss 19.735277\n",
      "iteration 400 / 2000: loss 16.006291\n",
      "iteration 500 / 2000: loss 13.223071\n",
      "iteration 600 / 2000: loss 10.957880\n",
      "iteration 700 / 2000: loss 9.194946\n",
      "iteration 800 / 2000: loss 7.692274\n",
      "iteration 900 / 2000: loss 6.499036\n",
      "iteration 1000 / 2000: loss 5.599679\n",
      "iteration 1100 / 2000: loss 4.791142\n",
      "iteration 1200 / 2000: loss 4.351603\n",
      "iteration 1300 / 2000: loss 3.718569\n",
      "iteration 1400 / 2000: loss 3.292566\n",
      "iteration 1500 / 2000: loss 3.104302\n",
      "iteration 1600 / 2000: loss 2.818546\n",
      "iteration 1700 / 2000: loss 2.619018\n",
      "iteration 1800 / 2000: loss 2.480216\n",
      "iteration 1900 / 2000: loss 2.403516\n",
      "iteration 0 / 2000: loss 46.824229\n",
      "iteration 100 / 2000: loss 34.477001\n",
      "iteration 200 / 2000: loss 26.584035\n",
      "iteration 300 / 2000: loss 20.765452\n",
      "iteration 400 / 2000: loss 16.309547\n",
      "iteration 500 / 2000: loss 12.849831\n",
      "iteration 600 / 2000: loss 10.185051\n",
      "iteration 700 / 2000: loss 8.295119\n",
      "iteration 800 / 2000: loss 6.797596\n",
      "iteration 900 / 2000: loss 5.629383\n",
      "iteration 1000 / 2000: loss 4.863149\n",
      "iteration 1100 / 2000: loss 4.089701\n",
      "iteration 1200 / 2000: loss 3.675555\n",
      "iteration 1300 / 2000: loss 3.100891\n",
      "iteration 1400 / 2000: loss 2.853091\n",
      "iteration 1500 / 2000: loss 2.540867\n",
      "iteration 1600 / 2000: loss 2.379848\n",
      "iteration 1700 / 2000: loss 2.285491\n",
      "iteration 1800 / 2000: loss 2.235290\n",
      "iteration 1900 / 2000: loss 2.059605\n",
      "iteration 0 / 2000: loss 53.939724\n",
      "iteration 100 / 2000: loss 38.252509\n",
      "iteration 200 / 2000: loss 28.151492\n",
      "iteration 300 / 2000: loss 21.075002\n",
      "iteration 400 / 2000: loss 16.095289\n",
      "iteration 500 / 2000: loss 12.244680\n",
      "iteration 600 / 2000: loss 9.584590\n",
      "iteration 700 / 2000: loss 7.437640\n",
      "iteration 800 / 2000: loss 5.975580\n",
      "iteration 900 / 2000: loss 4.922937\n",
      "iteration 1000 / 2000: loss 4.028530\n",
      "iteration 1100 / 2000: loss 3.554853\n",
      "iteration 1200 / 2000: loss 3.136164\n",
      "iteration 1300 / 2000: loss 2.685868\n",
      "iteration 1400 / 2000: loss 2.564630\n",
      "iteration 1500 / 2000: loss 2.346506\n",
      "iteration 1600 / 2000: loss 2.243511\n",
      "iteration 1700 / 2000: loss 2.079028\n",
      "iteration 1800 / 2000: loss 2.069387\n",
      "iteration 1900 / 2000: loss 2.073427\n",
      "iteration 0 / 2000: loss 61.129868\n",
      "iteration 100 / 2000: loss 41.651981\n",
      "iteration 200 / 2000: loss 29.494017\n",
      "iteration 300 / 2000: loss 21.144136\n",
      "iteration 400 / 2000: loss 15.329117\n",
      "iteration 500 / 2000: loss 11.452022\n",
      "iteration 600 / 2000: loss 8.670362\n",
      "iteration 700 / 2000: loss 6.561538\n",
      "iteration 800 / 2000: loss 5.138715\n",
      "iteration 900 / 2000: loss 4.208173\n",
      "iteration 1000 / 2000: loss 3.508046\n",
      "iteration 1100 / 2000: loss 2.994390\n",
      "iteration 1200 / 2000: loss 2.603377\n",
      "iteration 1300 / 2000: loss 2.442283\n",
      "iteration 1400 / 2000: loss 2.286039\n",
      "iteration 1500 / 2000: loss 2.173223\n",
      "iteration 1600 / 2000: loss 2.039432\n",
      "iteration 1700 / 2000: loss 2.027514\n",
      "iteration 1800 / 2000: loss 1.982942\n",
      "iteration 1900 / 2000: loss 1.925918\n",
      "iteration 0 / 2000: loss 65.515891\n",
      "iteration 100 / 2000: loss 43.888088\n",
      "iteration 200 / 2000: loss 29.754772\n",
      "iteration 300 / 2000: loss 20.570602\n",
      "iteration 400 / 2000: loss 14.501192\n",
      "iteration 500 / 2000: loss 10.393505\n",
      "iteration 600 / 2000: loss 7.580171\n",
      "iteration 700 / 2000: loss 5.765458\n",
      "iteration 800 / 2000: loss 4.423566\n",
      "iteration 900 / 2000: loss 3.682009\n",
      "iteration 1000 / 2000: loss 3.114129\n",
      "iteration 1100 / 2000: loss 2.595571\n",
      "iteration 1200 / 2000: loss 2.420832\n",
      "iteration 1300 / 2000: loss 2.188604\n",
      "iteration 1400 / 2000: loss 2.029629\n",
      "iteration 1500 / 2000: loss 2.032250\n",
      "iteration 1600 / 2000: loss 1.971438\n",
      "iteration 1700 / 2000: loss 2.009043\n",
      "iteration 1800 / 2000: loss 1.844420\n",
      "iteration 1900 / 2000: loss 1.935905\n",
      "iteration 0 / 2000: loss 7.338727\n",
      "iteration 100 / 2000: loss 3.391079\n",
      "iteration 200 / 2000: loss 3.116246\n",
      "iteration 300 / 2000: loss 2.835782\n",
      "iteration 400 / 2000: loss 2.490507\n",
      "iteration 500 / 2000: loss 2.517725\n",
      "iteration 600 / 2000: loss 2.591220\n",
      "iteration 700 / 2000: loss 2.516684\n",
      "iteration 800 / 2000: loss 2.487755\n",
      "iteration 900 / 2000: loss 2.224040\n",
      "iteration 1000 / 2000: loss 2.251957\n",
      "iteration 1100 / 2000: loss 2.485583\n",
      "iteration 1200 / 2000: loss 2.264339\n",
      "iteration 1300 / 2000: loss 2.486950\n",
      "iteration 1400 / 2000: loss 2.311201\n",
      "iteration 1500 / 2000: loss 2.220994\n",
      "iteration 1600 / 2000: loss 2.414580\n",
      "iteration 1700 / 2000: loss 2.063176\n",
      "iteration 1800 / 2000: loss 2.223191\n",
      "iteration 1900 / 2000: loss 2.305749\n",
      "iteration 0 / 2000: loss 12.204384\n",
      "iteration 100 / 2000: loss 9.641469\n",
      "iteration 200 / 2000: loss 8.737556\n",
      "iteration 300 / 2000: loss 8.115266\n",
      "iteration 400 / 2000: loss 7.792398\n",
      "iteration 500 / 2000: loss 7.343173\n",
      "iteration 600 / 2000: loss 6.719191\n",
      "iteration 700 / 2000: loss 6.517035\n",
      "iteration 800 / 2000: loss 6.031944\n",
      "iteration 900 / 2000: loss 5.715812\n",
      "iteration 1000 / 2000: loss 5.448940\n",
      "iteration 1100 / 2000: loss 5.336402\n",
      "iteration 1200 / 2000: loss 5.014286\n",
      "iteration 1300 / 2000: loss 4.949994\n",
      "iteration 1400 / 2000: loss 4.708292\n",
      "iteration 1500 / 2000: loss 4.340534\n",
      "iteration 1600 / 2000: loss 4.221898\n",
      "iteration 1700 / 2000: loss 4.055316\n",
      "iteration 1800 / 2000: loss 3.777816\n",
      "iteration 1900 / 2000: loss 3.832281\n",
      "iteration 0 / 2000: loss 20.100358\n",
      "iteration 100 / 2000: loss 15.057769\n",
      "iteration 200 / 2000: loss 13.120620\n",
      "iteration 300 / 2000: loss 11.694073\n",
      "iteration 400 / 2000: loss 10.341692\n",
      "iteration 500 / 2000: loss 9.393087\n",
      "iteration 600 / 2000: loss 8.430447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 2000: loss 7.587212\n",
      "iteration 800 / 2000: loss 6.866475\n",
      "iteration 900 / 2000: loss 6.327231\n",
      "iteration 1000 / 2000: loss 5.693651\n",
      "iteration 1100 / 2000: loss 5.242213\n",
      "iteration 1200 / 2000: loss 4.954659\n",
      "iteration 1300 / 2000: loss 4.535040\n",
      "iteration 1400 / 2000: loss 4.153812\n",
      "iteration 1500 / 2000: loss 3.956360\n",
      "iteration 1600 / 2000: loss 3.567173\n",
      "iteration 1700 / 2000: loss 3.491076\n",
      "iteration 1800 / 2000: loss 3.314182\n",
      "iteration 1900 / 2000: loss 3.007162\n",
      "iteration 0 / 2000: loss 25.900796\n",
      "iteration 100 / 2000: loss 19.719248\n",
      "iteration 200 / 2000: loss 16.356507\n",
      "iteration 300 / 2000: loss 13.776464\n",
      "iteration 400 / 2000: loss 11.819668\n",
      "iteration 500 / 2000: loss 9.906270\n",
      "iteration 600 / 2000: loss 8.549046\n",
      "iteration 700 / 2000: loss 7.337500\n",
      "iteration 800 / 2000: loss 6.333432\n",
      "iteration 900 / 2000: loss 5.699619\n",
      "iteration 1000 / 2000: loss 4.973514\n",
      "iteration 1100 / 2000: loss 4.303744\n",
      "iteration 1200 / 2000: loss 4.116774\n",
      "iteration 1300 / 2000: loss 3.703035\n",
      "iteration 1400 / 2000: loss 3.308017\n",
      "iteration 1500 / 2000: loss 2.989031\n",
      "iteration 1600 / 2000: loss 2.939819\n",
      "iteration 1700 / 2000: loss 2.682497\n",
      "iteration 1800 / 2000: loss 2.441600\n",
      "iteration 1900 / 2000: loss 2.402513\n",
      "iteration 0 / 2000: loss 32.847232\n",
      "iteration 100 / 2000: loss 24.273356\n",
      "iteration 200 / 2000: loss 19.105980\n",
      "iteration 300 / 2000: loss 15.121360\n",
      "iteration 400 / 2000: loss 12.192476\n",
      "iteration 500 / 2000: loss 10.008810\n",
      "iteration 600 / 2000: loss 8.048700\n",
      "iteration 700 / 2000: loss 6.789710\n",
      "iteration 800 / 2000: loss 5.697668\n",
      "iteration 900 / 2000: loss 4.861376\n",
      "iteration 1000 / 2000: loss 4.173967\n",
      "iteration 1100 / 2000: loss 3.655310\n",
      "iteration 1200 / 2000: loss 3.346143\n",
      "iteration 1300 / 2000: loss 2.880797\n",
      "iteration 1400 / 2000: loss 2.608747\n",
      "iteration 1500 / 2000: loss 2.622087\n",
      "iteration 1600 / 2000: loss 2.369607\n",
      "iteration 1700 / 2000: loss 2.210009\n",
      "iteration 1800 / 2000: loss 2.192931\n",
      "iteration 1900 / 2000: loss 2.044542\n",
      "iteration 0 / 2000: loss 40.078063\n",
      "iteration 100 / 2000: loss 27.844153\n",
      "iteration 200 / 2000: loss 20.506868\n",
      "iteration 300 / 2000: loss 15.741723\n",
      "iteration 400 / 2000: loss 11.843438\n",
      "iteration 500 / 2000: loss 9.243139\n",
      "iteration 600 / 2000: loss 7.042892\n",
      "iteration 700 / 2000: loss 5.827656\n",
      "iteration 800 / 2000: loss 4.746017\n",
      "iteration 900 / 2000: loss 3.868849\n",
      "iteration 1000 / 2000: loss 3.415406\n",
      "iteration 1100 / 2000: loss 2.951224\n",
      "iteration 1200 / 2000: loss 2.714338\n",
      "iteration 1300 / 2000: loss 2.502366\n",
      "iteration 1400 / 2000: loss 2.302172\n",
      "iteration 1500 / 2000: loss 2.124670\n",
      "iteration 1600 / 2000: loss 2.129802\n",
      "iteration 1700 / 2000: loss 2.003335\n",
      "iteration 1800 / 2000: loss 1.932606\n",
      "iteration 1900 / 2000: loss 1.934416\n",
      "iteration 0 / 2000: loss 47.421441\n",
      "iteration 100 / 2000: loss 31.306594\n",
      "iteration 200 / 2000: loss 21.860896\n",
      "iteration 300 / 2000: loss 15.610643\n",
      "iteration 400 / 2000: loss 11.189224\n",
      "iteration 500 / 2000: loss 8.451437\n",
      "iteration 600 / 2000: loss 6.346087\n",
      "iteration 700 / 2000: loss 4.978251\n",
      "iteration 800 / 2000: loss 3.968743\n",
      "iteration 900 / 2000: loss 3.355426\n",
      "iteration 1000 / 2000: loss 2.929832\n",
      "iteration 1100 / 2000: loss 2.534800\n",
      "iteration 1200 / 2000: loss 2.405902\n",
      "iteration 1300 / 2000: loss 2.153437\n",
      "iteration 1400 / 2000: loss 2.129574\n",
      "iteration 1500 / 2000: loss 2.017377\n",
      "iteration 1600 / 2000: loss 1.956106\n",
      "iteration 1700 / 2000: loss 1.967397\n",
      "iteration 1800 / 2000: loss 1.863054\n",
      "iteration 1900 / 2000: loss 1.928362\n",
      "iteration 0 / 2000: loss 52.593115\n",
      "iteration 100 / 2000: loss 33.143937\n",
      "iteration 200 / 2000: loss 22.173067\n",
      "iteration 300 / 2000: loss 15.028921\n",
      "iteration 400 / 2000: loss 10.285022\n",
      "iteration 500 / 2000: loss 7.391910\n",
      "iteration 600 / 2000: loss 5.396612\n",
      "iteration 700 / 2000: loss 4.157165\n",
      "iteration 800 / 2000: loss 3.312056\n",
      "iteration 900 / 2000: loss 2.744494\n",
      "iteration 1000 / 2000: loss 2.610150\n",
      "iteration 1100 / 2000: loss 2.367414\n",
      "iteration 1200 / 2000: loss 2.147801\n",
      "iteration 1300 / 2000: loss 2.008478\n",
      "iteration 1400 / 2000: loss 1.982798\n",
      "iteration 1500 / 2000: loss 1.963418\n",
      "iteration 1600 / 2000: loss 1.946928\n",
      "iteration 1700 / 2000: loss 1.944251\n",
      "iteration 1800 / 2000: loss 1.914158\n",
      "iteration 1900 / 2000: loss 1.877154\n",
      "iteration 0 / 2000: loss 59.177013\n",
      "iteration 100 / 2000: loss 35.693167\n",
      "iteration 200 / 2000: loss 22.394820\n",
      "iteration 300 / 2000: loss 14.408669\n",
      "iteration 400 / 2000: loss 9.453065\n",
      "iteration 500 / 2000: loss 6.510514\n",
      "iteration 600 / 2000: loss 4.728161\n",
      "iteration 700 / 2000: loss 3.656361\n",
      "iteration 800 / 2000: loss 3.000915\n",
      "iteration 900 / 2000: loss 2.542381\n",
      "iteration 1000 / 2000: loss 2.232180\n",
      "iteration 1100 / 2000: loss 2.069156\n",
      "iteration 1200 / 2000: loss 2.083578\n",
      "iteration 1300 / 2000: loss 2.013236\n",
      "iteration 1400 / 2000: loss 1.821809\n",
      "iteration 1500 / 2000: loss 1.888816\n",
      "iteration 1600 / 2000: loss 1.915178\n",
      "iteration 1700 / 2000: loss 1.830448\n",
      "iteration 1800 / 2000: loss 1.836221\n",
      "iteration 1900 / 2000: loss 1.890471\n",
      "iteration 0 / 2000: loss 67.073550\n",
      "iteration 100 / 2000: loss 37.618514\n",
      "iteration 200 / 2000: loss 22.468091\n",
      "iteration 300 / 2000: loss 13.748370\n",
      "iteration 400 / 2000: loss 8.664345\n",
      "iteration 500 / 2000: loss 5.704709\n",
      "iteration 600 / 2000: loss 4.127656\n",
      "iteration 700 / 2000: loss 3.226604\n",
      "iteration 800 / 2000: loss 2.643704\n",
      "iteration 900 / 2000: loss 2.325008\n",
      "iteration 1000 / 2000: loss 2.234062\n",
      "iteration 1100 / 2000: loss 2.026665\n",
      "iteration 1200 / 2000: loss 1.990134\n",
      "iteration 1300 / 2000: loss 1.978700\n",
      "iteration 1400 / 2000: loss 1.941671\n",
      "iteration 1500 / 2000: loss 1.945472\n",
      "iteration 1600 / 2000: loss 1.872209\n",
      "iteration 1700 / 2000: loss 1.878557\n",
      "iteration 1800 / 2000: loss 2.033490\n",
      "iteration 1900 / 2000: loss 1.877506\n",
      "iteration 0 / 2000: loss 5.611863\n",
      "iteration 100 / 2000: loss 2.930982\n",
      "iteration 200 / 2000: loss 2.938256\n",
      "iteration 300 / 2000: loss 2.684262\n",
      "iteration 400 / 2000: loss 2.616598\n",
      "iteration 500 / 2000: loss 2.598120\n",
      "iteration 600 / 2000: loss 2.433601\n",
      "iteration 700 / 2000: loss 2.506064\n",
      "iteration 800 / 2000: loss 2.663541\n",
      "iteration 900 / 2000: loss 2.350606\n",
      "iteration 1000 / 2000: loss 2.436049\n",
      "iteration 1100 / 2000: loss 2.322992\n",
      "iteration 1200 / 2000: loss 2.571621\n",
      "iteration 1300 / 2000: loss 2.295441\n",
      "iteration 1400 / 2000: loss 2.053028\n",
      "iteration 1500 / 2000: loss 2.217612\n",
      "iteration 1600 / 2000: loss 2.103283\n",
      "iteration 1700 / 2000: loss 2.161872\n",
      "iteration 1800 / 2000: loss 2.225685\n",
      "iteration 1900 / 2000: loss 2.112214\n",
      "iteration 0 / 2000: loss 12.371323\n",
      "iteration 100 / 2000: loss 9.323735\n",
      "iteration 200 / 2000: loss 8.589187\n",
      "iteration 300 / 2000: loss 7.848815\n",
      "iteration 400 / 2000: loss 7.290473\n",
      "iteration 500 / 2000: loss 6.755020\n",
      "iteration 600 / 2000: loss 6.334143\n",
      "iteration 700 / 2000: loss 5.866494\n",
      "iteration 800 / 2000: loss 5.576961\n",
      "iteration 900 / 2000: loss 5.234413\n",
      "iteration 1000 / 2000: loss 4.869402\n",
      "iteration 1100 / 2000: loss 4.705956\n",
      "iteration 1200 / 2000: loss 4.469191\n",
      "iteration 1300 / 2000: loss 4.254893\n",
      "iteration 1400 / 2000: loss 4.034366\n",
      "iteration 1500 / 2000: loss 3.787564\n",
      "iteration 1600 / 2000: loss 3.602876\n",
      "iteration 1700 / 2000: loss 3.522539\n",
      "iteration 1800 / 2000: loss 3.399847\n",
      "iteration 1900 / 2000: loss 3.132930\n",
      "iteration 0 / 2000: loss 20.201129\n",
      "iteration 100 / 2000: loss 14.373004\n",
      "iteration 200 / 2000: loss 12.250099\n",
      "iteration 300 / 2000: loss 10.656780\n",
      "iteration 400 / 2000: loss 9.480462\n",
      "iteration 500 / 2000: loss 8.111950\n",
      "iteration 600 / 2000: loss 7.103041\n",
      "iteration 700 / 2000: loss 6.458528\n",
      "iteration 800 / 2000: loss 5.788062\n",
      "iteration 900 / 2000: loss 5.094018\n",
      "iteration 1000 / 2000: loss 4.529266\n",
      "iteration 1100 / 2000: loss 4.168467\n",
      "iteration 1200 / 2000: loss 3.700835\n",
      "iteration 1300 / 2000: loss 3.425284\n",
      "iteration 1400 / 2000: loss 3.268645\n",
      "iteration 1500 / 2000: loss 3.099213\n",
      "iteration 1600 / 2000: loss 2.851080\n",
      "iteration 1700 / 2000: loss 2.534676\n",
      "iteration 1800 / 2000: loss 2.531862\n",
      "iteration 1900 / 2000: loss 2.454775\n",
      "iteration 0 / 2000: loss 26.883224\n",
      "iteration 100 / 2000: loss 19.019851\n",
      "iteration 200 / 2000: loss 15.178980\n",
      "iteration 300 / 2000: loss 12.143694\n",
      "iteration 400 / 2000: loss 9.964540\n",
      "iteration 500 / 2000: loss 8.129033\n",
      "iteration 600 / 2000: loss 6.722505\n",
      "iteration 700 / 2000: loss 5.759615\n",
      "iteration 800 / 2000: loss 4.829357\n",
      "iteration 900 / 2000: loss 4.246200\n",
      "iteration 1000 / 2000: loss 3.819357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 2000: loss 3.293889\n",
      "iteration 1200 / 2000: loss 2.965926\n",
      "iteration 1300 / 2000: loss 2.629193\n",
      "iteration 1400 / 2000: loss 2.513704\n",
      "iteration 1500 / 2000: loss 2.388926\n",
      "iteration 1600 / 2000: loss 2.281188\n",
      "iteration 1700 / 2000: loss 2.167973\n",
      "iteration 1800 / 2000: loss 1.966818\n",
      "iteration 1900 / 2000: loss 2.101010\n",
      "iteration 0 / 2000: loss 33.753078\n",
      "iteration 100 / 2000: loss 22.652186\n",
      "iteration 200 / 2000: loss 16.735123\n",
      "iteration 300 / 2000: loss 12.488645\n",
      "iteration 400 / 2000: loss 9.743467\n",
      "iteration 500 / 2000: loss 7.594869\n",
      "iteration 600 / 2000: loss 5.920269\n",
      "iteration 700 / 2000: loss 4.753866\n",
      "iteration 800 / 2000: loss 3.957601\n",
      "iteration 900 / 2000: loss 3.330851\n",
      "iteration 1000 / 2000: loss 3.025875\n",
      "iteration 1100 / 2000: loss 2.617520\n",
      "iteration 1200 / 2000: loss 2.410278\n",
      "iteration 1300 / 2000: loss 2.249566\n",
      "iteration 1400 / 2000: loss 2.238070\n",
      "iteration 1500 / 2000: loss 2.106134\n",
      "iteration 1600 / 2000: loss 1.992516\n",
      "iteration 1700 / 2000: loss 1.998925\n",
      "iteration 1800 / 2000: loss 1.870707\n",
      "iteration 1900 / 2000: loss 1.950793\n",
      "iteration 0 / 2000: loss 40.099622\n",
      "iteration 100 / 2000: loss 25.069796\n",
      "iteration 200 / 2000: loss 17.395996\n",
      "iteration 300 / 2000: loss 12.462744\n",
      "iteration 400 / 2000: loss 8.872093\n",
      "iteration 500 / 2000: loss 6.616023\n",
      "iteration 600 / 2000: loss 5.023211\n",
      "iteration 700 / 2000: loss 4.116634\n",
      "iteration 800 / 2000: loss 3.266142\n",
      "iteration 900 / 2000: loss 2.807607\n",
      "iteration 1000 / 2000: loss 2.377702\n",
      "iteration 1100 / 2000: loss 2.333345\n",
      "iteration 1200 / 2000: loss 2.117777\n",
      "iteration 1300 / 2000: loss 2.063439\n",
      "iteration 1400 / 2000: loss 1.986956\n",
      "iteration 1500 / 2000: loss 1.944191\n",
      "iteration 1600 / 2000: loss 1.880940\n",
      "iteration 1700 / 2000: loss 1.839913\n",
      "iteration 1800 / 2000: loss 1.900103\n",
      "iteration 1900 / 2000: loss 1.957189\n",
      "iteration 0 / 2000: loss 47.347085\n",
      "iteration 100 / 2000: loss 28.196830\n",
      "iteration 200 / 2000: loss 18.120265\n",
      "iteration 300 / 2000: loss 12.026521\n",
      "iteration 400 / 2000: loss 8.148163\n",
      "iteration 500 / 2000: loss 5.737789\n",
      "iteration 600 / 2000: loss 4.321366\n",
      "iteration 700 / 2000: loss 3.328756\n",
      "iteration 800 / 2000: loss 2.854437\n",
      "iteration 900 / 2000: loss 2.477242\n",
      "iteration 1000 / 2000: loss 2.211933\n",
      "iteration 1100 / 2000: loss 2.035762\n",
      "iteration 1200 / 2000: loss 1.990110\n",
      "iteration 1300 / 2000: loss 1.862854\n",
      "iteration 1400 / 2000: loss 1.960188\n",
      "iteration 1500 / 2000: loss 1.898171\n",
      "iteration 1600 / 2000: loss 1.804696\n",
      "iteration 1700 / 2000: loss 1.865843\n",
      "iteration 1800 / 2000: loss 1.847179\n",
      "iteration 1900 / 2000: loss 1.871539\n",
      "iteration 0 / 2000: loss 54.071245\n",
      "iteration 100 / 2000: loss 30.505432\n",
      "iteration 200 / 2000: loss 18.193263\n",
      "iteration 300 / 2000: loss 11.245025\n",
      "iteration 400 / 2000: loss 7.285394\n",
      "iteration 500 / 2000: loss 5.015713\n",
      "iteration 600 / 2000: loss 3.693572\n",
      "iteration 700 / 2000: loss 2.842633\n",
      "iteration 800 / 2000: loss 2.559459\n",
      "iteration 900 / 2000: loss 2.221956\n",
      "iteration 1000 / 2000: loss 2.111193\n",
      "iteration 1100 / 2000: loss 1.996138\n",
      "iteration 1200 / 2000: loss 1.917103\n",
      "iteration 1300 / 2000: loss 1.886843\n",
      "iteration 1400 / 2000: loss 1.986008\n",
      "iteration 1500 / 2000: loss 1.878249\n",
      "iteration 1600 / 2000: loss 1.889105\n",
      "iteration 1700 / 2000: loss 1.920621\n",
      "iteration 1800 / 2000: loss 1.749743\n",
      "iteration 1900 / 2000: loss 1.905652\n",
      "iteration 0 / 2000: loss 59.788137\n",
      "iteration 100 / 2000: loss 31.385485\n",
      "iteration 200 / 2000: loss 17.505404\n",
      "iteration 300 / 2000: loss 10.369319\n",
      "iteration 400 / 2000: loss 6.384698\n",
      "iteration 500 / 2000: loss 4.254794\n",
      "iteration 600 / 2000: loss 3.072680\n",
      "iteration 700 / 2000: loss 2.557459\n",
      "iteration 800 / 2000: loss 2.308053\n",
      "iteration 900 / 2000: loss 2.167230\n",
      "iteration 1000 / 2000: loss 1.826621\n",
      "iteration 1100 / 2000: loss 1.981377\n",
      "iteration 1200 / 2000: loss 1.847548\n",
      "iteration 1300 / 2000: loss 1.912592\n",
      "iteration 1400 / 2000: loss 1.844517\n",
      "iteration 1500 / 2000: loss 1.848267\n",
      "iteration 1600 / 2000: loss 1.801758\n",
      "iteration 1700 / 2000: loss 1.982165\n",
      "iteration 1800 / 2000: loss 1.825716\n",
      "iteration 1900 / 2000: loss 1.871613\n",
      "iteration 0 / 2000: loss 67.910422\n",
      "iteration 100 / 2000: loss 32.806668\n",
      "iteration 200 / 2000: loss 16.951661\n",
      "iteration 300 / 2000: loss 9.446585\n",
      "iteration 400 / 2000: loss 5.646734\n",
      "iteration 500 / 2000: loss 3.761055\n",
      "iteration 600 / 2000: loss 2.688444\n",
      "iteration 700 / 2000: loss 2.392686\n",
      "iteration 800 / 2000: loss 2.137615\n",
      "iteration 900 / 2000: loss 2.001103\n",
      "iteration 1000 / 2000: loss 1.963977\n",
      "iteration 1100 / 2000: loss 1.821005\n",
      "iteration 1200 / 2000: loss 1.898425\n",
      "iteration 1300 / 2000: loss 1.970995\n",
      "iteration 1400 / 2000: loss 1.777863\n",
      "iteration 1500 / 2000: loss 1.867450\n",
      "iteration 1600 / 2000: loss 1.880245\n",
      "iteration 1700 / 2000: loss 1.882911\n",
      "iteration 1800 / 2000: loss 1.875624\n",
      "iteration 1900 / 2000: loss 1.877023\n",
      "iteration 0 / 2000: loss 5.749541\n",
      "iteration 100 / 2000: loss 2.973120\n",
      "iteration 200 / 2000: loss 2.832133\n",
      "iteration 300 / 2000: loss 2.451480\n",
      "iteration 400 / 2000: loss 2.435043\n",
      "iteration 500 / 2000: loss 2.323820\n",
      "iteration 600 / 2000: loss 2.443407\n",
      "iteration 700 / 2000: loss 2.289245\n",
      "iteration 800 / 2000: loss 2.297085\n",
      "iteration 900 / 2000: loss 2.328327\n",
      "iteration 1000 / 2000: loss 2.302764\n",
      "iteration 1100 / 2000: loss 2.011989\n",
      "iteration 1200 / 2000: loss 2.298111\n",
      "iteration 1300 / 2000: loss 1.968851\n",
      "iteration 1400 / 2000: loss 2.078351\n",
      "iteration 1500 / 2000: loss 2.161946\n",
      "iteration 1600 / 2000: loss 2.243133\n",
      "iteration 1700 / 2000: loss 1.902890\n",
      "iteration 1800 / 2000: loss 2.037610\n",
      "iteration 1900 / 2000: loss 2.112421\n",
      "iteration 0 / 2000: loss 12.011133\n",
      "iteration 100 / 2000: loss 8.738845\n",
      "iteration 200 / 2000: loss 7.662465\n",
      "iteration 300 / 2000: loss 7.269335\n",
      "iteration 400 / 2000: loss 6.553632\n",
      "iteration 500 / 2000: loss 6.033387\n",
      "iteration 600 / 2000: loss 5.747462\n",
      "iteration 700 / 2000: loss 5.331283\n",
      "iteration 800 / 2000: loss 4.985436\n",
      "iteration 900 / 2000: loss 4.595618\n",
      "iteration 1000 / 2000: loss 4.291727\n",
      "iteration 1100 / 2000: loss 4.019490\n",
      "iteration 1200 / 2000: loss 3.720979\n",
      "iteration 1300 / 2000: loss 3.592207\n",
      "iteration 1400 / 2000: loss 3.418233\n",
      "iteration 1500 / 2000: loss 3.176773\n",
      "iteration 1600 / 2000: loss 3.062785\n",
      "iteration 1700 / 2000: loss 3.068817\n",
      "iteration 1800 / 2000: loss 2.850216\n",
      "iteration 1900 / 2000: loss 2.892023\n",
      "iteration 0 / 2000: loss 19.895213\n",
      "iteration 100 / 2000: loss 13.615837\n",
      "iteration 200 / 2000: loss 11.419061\n",
      "iteration 300 / 2000: loss 9.538637\n",
      "iteration 400 / 2000: loss 8.056399\n",
      "iteration 500 / 2000: loss 7.067943\n",
      "iteration 600 / 2000: loss 6.204286\n",
      "iteration 700 / 2000: loss 5.353885\n",
      "iteration 800 / 2000: loss 4.593555\n",
      "iteration 900 / 2000: loss 4.162248\n",
      "iteration 1000 / 2000: loss 3.724033\n",
      "iteration 1100 / 2000: loss 3.254277\n",
      "iteration 1200 / 2000: loss 3.072115\n",
      "iteration 1300 / 2000: loss 2.764830\n",
      "iteration 1400 / 2000: loss 2.639154\n",
      "iteration 1500 / 2000: loss 2.574818\n",
      "iteration 1600 / 2000: loss 2.387394\n",
      "iteration 1700 / 2000: loss 2.281158\n",
      "iteration 1800 / 2000: loss 2.123500\n",
      "iteration 1900 / 2000: loss 2.122327\n",
      "iteration 0 / 2000: loss 25.552592\n",
      "iteration 100 / 2000: loss 17.879489\n",
      "iteration 200 / 2000: loss 13.640857\n",
      "iteration 300 / 2000: loss 10.613990\n",
      "iteration 400 / 2000: loss 8.455879\n",
      "iteration 500 / 2000: loss 6.788850\n",
      "iteration 600 / 2000: loss 5.340626\n",
      "iteration 700 / 2000: loss 4.641015\n",
      "iteration 800 / 2000: loss 3.882162\n",
      "iteration 900 / 2000: loss 3.281314\n",
      "iteration 1000 / 2000: loss 2.898715\n",
      "iteration 1100 / 2000: loss 2.691501\n",
      "iteration 1200 / 2000: loss 2.526865\n",
      "iteration 1300 / 2000: loss 2.276149\n",
      "iteration 1400 / 2000: loss 2.256104\n",
      "iteration 1500 / 2000: loss 2.074380\n",
      "iteration 1600 / 2000: loss 2.022395\n",
      "iteration 1700 / 2000: loss 2.009523\n",
      "iteration 1800 / 2000: loss 1.984106\n",
      "iteration 1900 / 2000: loss 1.935450\n",
      "iteration 0 / 2000: loss 32.427389\n",
      "iteration 100 / 2000: loss 20.911979\n",
      "iteration 200 / 2000: loss 14.505007\n",
      "iteration 300 / 2000: loss 10.370368\n",
      "iteration 400 / 2000: loss 7.708135\n",
      "iteration 500 / 2000: loss 5.754392\n",
      "iteration 600 / 2000: loss 4.597511\n",
      "iteration 700 / 2000: loss 3.722083\n",
      "iteration 800 / 2000: loss 3.129023\n",
      "iteration 900 / 2000: loss 2.649603\n",
      "iteration 1000 / 2000: loss 2.376030\n",
      "iteration 1100 / 2000: loss 2.199169\n",
      "iteration 1200 / 2000: loss 2.134841\n",
      "iteration 1300 / 2000: loss 1.978634\n",
      "iteration 1400 / 2000: loss 1.919868\n",
      "iteration 1500 / 2000: loss 1.950320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600 / 2000: loss 1.951807\n",
      "iteration 1700 / 2000: loss 1.768931\n",
      "iteration 1800 / 2000: loss 1.838175\n",
      "iteration 1900 / 2000: loss 1.893974\n",
      "iteration 0 / 2000: loss 40.346409\n",
      "iteration 100 / 2000: loss 23.590170\n",
      "iteration 200 / 2000: loss 15.076320\n",
      "iteration 300 / 2000: loss 9.922978\n",
      "iteration 400 / 2000: loss 6.854413\n",
      "iteration 500 / 2000: loss 4.972800\n",
      "iteration 600 / 2000: loss 3.757084\n",
      "iteration 700 / 2000: loss 3.127582\n",
      "iteration 800 / 2000: loss 2.503225\n",
      "iteration 900 / 2000: loss 2.321769\n",
      "iteration 1000 / 2000: loss 2.248616\n",
      "iteration 1100 / 2000: loss 2.006110\n",
      "iteration 1200 / 2000: loss 1.871207\n",
      "iteration 1300 / 2000: loss 1.933123\n",
      "iteration 1400 / 2000: loss 1.785304\n",
      "iteration 1500 / 2000: loss 1.897424\n",
      "iteration 1600 / 2000: loss 1.872429\n",
      "iteration 1700 / 2000: loss 1.774050\n",
      "iteration 1800 / 2000: loss 1.907730\n",
      "iteration 1900 / 2000: loss 1.819064\n",
      "iteration 0 / 2000: loss 47.001073\n",
      "iteration 100 / 2000: loss 25.580983\n",
      "iteration 200 / 2000: loss 15.057293\n",
      "iteration 300 / 2000: loss 9.197047\n",
      "iteration 400 / 2000: loss 5.949547\n",
      "iteration 500 / 2000: loss 4.231592\n",
      "iteration 600 / 2000: loss 3.268727\n",
      "iteration 700 / 2000: loss 2.634623\n",
      "iteration 800 / 2000: loss 2.230903\n",
      "iteration 900 / 2000: loss 2.106088\n",
      "iteration 1000 / 2000: loss 2.004608\n",
      "iteration 1100 / 2000: loss 1.959653\n",
      "iteration 1200 / 2000: loss 1.946797\n",
      "iteration 1300 / 2000: loss 1.965078\n",
      "iteration 1400 / 2000: loss 1.886042\n",
      "iteration 1500 / 2000: loss 1.804372\n",
      "iteration 1600 / 2000: loss 1.778746\n",
      "iteration 1700 / 2000: loss 1.745991\n",
      "iteration 1800 / 2000: loss 1.819457\n",
      "iteration 1900 / 2000: loss 1.792827\n",
      "iteration 0 / 2000: loss 53.982512\n",
      "iteration 100 / 2000: loss 26.845820\n",
      "iteration 200 / 2000: loss 14.722504\n",
      "iteration 300 / 2000: loss 8.357981\n",
      "iteration 400 / 2000: loss 5.228898\n",
      "iteration 500 / 2000: loss 3.550283\n",
      "iteration 600 / 2000: loss 2.756840\n",
      "iteration 700 / 2000: loss 2.292470\n",
      "iteration 800 / 2000: loss 2.093543\n",
      "iteration 900 / 2000: loss 1.969038\n",
      "iteration 1000 / 2000: loss 1.964295\n",
      "iteration 1100 / 2000: loss 1.871530\n",
      "iteration 1200 / 2000: loss 1.965782\n",
      "iteration 1300 / 2000: loss 1.767885\n",
      "iteration 1400 / 2000: loss 1.899685\n",
      "iteration 1500 / 2000: loss 1.806821\n",
      "iteration 1600 / 2000: loss 1.815937\n",
      "iteration 1700 / 2000: loss 1.789865\n",
      "iteration 1800 / 2000: loss 1.810939\n",
      "iteration 1900 / 2000: loss 1.899025\n",
      "iteration 0 / 2000: loss 58.843386\n",
      "iteration 100 / 2000: loss 27.588005\n",
      "iteration 200 / 2000: loss 13.896207\n",
      "iteration 300 / 2000: loss 7.435209\n",
      "iteration 400 / 2000: loss 4.495449\n",
      "iteration 500 / 2000: loss 3.117525\n",
      "iteration 600 / 2000: loss 2.564896\n",
      "iteration 700 / 2000: loss 2.141751\n",
      "iteration 800 / 2000: loss 1.928931\n",
      "iteration 900 / 2000: loss 1.874278\n",
      "iteration 1000 / 2000: loss 1.868804\n",
      "iteration 1100 / 2000: loss 1.983036\n",
      "iteration 1200 / 2000: loss 1.959083\n",
      "iteration 1300 / 2000: loss 1.937108\n",
      "iteration 1400 / 2000: loss 1.878969\n",
      "iteration 1500 / 2000: loss 1.806043\n",
      "iteration 1600 / 2000: loss 1.842181\n",
      "iteration 1700 / 2000: loss 1.837946\n",
      "iteration 1800 / 2000: loss 1.867631\n",
      "iteration 1900 / 2000: loss 1.798762\n",
      "iteration 0 / 2000: loss 65.962314\n",
      "iteration 100 / 2000: loss 27.802541\n",
      "iteration 200 / 2000: loss 12.833625\n",
      "iteration 300 / 2000: loss 6.673087\n",
      "iteration 400 / 2000: loss 3.875296\n",
      "iteration 500 / 2000: loss 2.819881\n",
      "iteration 600 / 2000: loss 2.173107\n",
      "iteration 700 / 2000: loss 2.013264\n",
      "iteration 800 / 2000: loss 2.037545\n",
      "iteration 900 / 2000: loss 1.812429\n",
      "iteration 1000 / 2000: loss 1.871632\n",
      "iteration 1100 / 2000: loss 1.876326\n",
      "iteration 1200 / 2000: loss 1.971171\n",
      "iteration 1300 / 2000: loss 1.911082\n",
      "iteration 1400 / 2000: loss 1.875302\n",
      "iteration 1500 / 2000: loss 1.910862\n",
      "iteration 1600 / 2000: loss 1.908671\n",
      "iteration 1700 / 2000: loss 1.906823\n",
      "iteration 1800 / 2000: loss 1.909400\n",
      "iteration 1900 / 2000: loss 1.843400\n",
      "iteration 0 / 2000: loss 6.340103\n",
      "iteration 100 / 2000: loss 2.879244\n",
      "iteration 200 / 2000: loss 2.576888\n",
      "iteration 300 / 2000: loss 2.784676\n",
      "iteration 400 / 2000: loss 2.644577\n",
      "iteration 500 / 2000: loss 2.180261\n",
      "iteration 600 / 2000: loss 2.294851\n",
      "iteration 700 / 2000: loss 2.249287\n",
      "iteration 800 / 2000: loss 2.319437\n",
      "iteration 900 / 2000: loss 2.176334\n",
      "iteration 1000 / 2000: loss 2.290770\n",
      "iteration 1100 / 2000: loss 2.129869\n",
      "iteration 1200 / 2000: loss 2.308461\n",
      "iteration 1300 / 2000: loss 2.230288\n",
      "iteration 1400 / 2000: loss 2.189623\n",
      "iteration 1500 / 2000: loss 2.154224\n",
      "iteration 1600 / 2000: loss 2.198457\n",
      "iteration 1700 / 2000: loss 2.132358\n",
      "iteration 1800 / 2000: loss 2.208263\n",
      "iteration 1900 / 2000: loss 2.163938\n",
      "iteration 0 / 2000: loss 12.166037\n",
      "iteration 100 / 2000: loss 8.867132\n",
      "iteration 200 / 2000: loss 7.851829\n",
      "iteration 300 / 2000: loss 7.073942\n",
      "iteration 400 / 2000: loss 6.286019\n",
      "iteration 500 / 2000: loss 5.810298\n",
      "iteration 600 / 2000: loss 5.126176\n",
      "iteration 700 / 2000: loss 4.997927\n",
      "iteration 800 / 2000: loss 4.599816\n",
      "iteration 900 / 2000: loss 4.147360\n",
      "iteration 1000 / 2000: loss 3.955642\n",
      "iteration 1100 / 2000: loss 3.665358\n",
      "iteration 1200 / 2000: loss 3.485473\n",
      "iteration 1300 / 2000: loss 3.288449\n",
      "iteration 1400 / 2000: loss 3.059221\n",
      "iteration 1500 / 2000: loss 2.884493\n",
      "iteration 1600 / 2000: loss 2.905317\n",
      "iteration 1700 / 2000: loss 2.619840\n",
      "iteration 1800 / 2000: loss 2.493792\n",
      "iteration 1900 / 2000: loss 2.529710\n",
      "iteration 0 / 2000: loss 20.083711\n",
      "iteration 100 / 2000: loss 13.362917\n",
      "iteration 200 / 2000: loss 10.757322\n",
      "iteration 300 / 2000: loss 8.967417\n",
      "iteration 400 / 2000: loss 7.285265\n",
      "iteration 500 / 2000: loss 6.234463\n",
      "iteration 600 / 2000: loss 5.478747\n",
      "iteration 700 / 2000: loss 4.577636\n",
      "iteration 800 / 2000: loss 3.936883\n",
      "iteration 900 / 2000: loss 3.491806\n",
      "iteration 1000 / 2000: loss 3.011086\n",
      "iteration 1100 / 2000: loss 2.979200\n",
      "iteration 1200 / 2000: loss 2.695552\n",
      "iteration 1300 / 2000: loss 2.520505\n",
      "iteration 1400 / 2000: loss 2.344131\n",
      "iteration 1500 / 2000: loss 2.168111\n",
      "iteration 1600 / 2000: loss 2.234066\n",
      "iteration 1700 / 2000: loss 2.097622\n",
      "iteration 1800 / 2000: loss 1.964567\n",
      "iteration 1900 / 2000: loss 1.917505\n",
      "iteration 0 / 2000: loss 26.448197\n",
      "iteration 100 / 2000: loss 16.873761\n",
      "iteration 200 / 2000: loss 12.298991\n",
      "iteration 300 / 2000: loss 9.384030\n",
      "iteration 400 / 2000: loss 7.096680\n",
      "iteration 500 / 2000: loss 5.543913\n",
      "iteration 600 / 2000: loss 4.451569\n",
      "iteration 700 / 2000: loss 3.636593\n",
      "iteration 800 / 2000: loss 3.057616\n",
      "iteration 900 / 2000: loss 2.784124\n",
      "iteration 1000 / 2000: loss 2.439839\n",
      "iteration 1100 / 2000: loss 2.337309\n",
      "iteration 1200 / 2000: loss 2.204836\n",
      "iteration 1300 / 2000: loss 1.969043\n",
      "iteration 1400 / 2000: loss 1.873256\n",
      "iteration 1500 / 2000: loss 1.976024\n",
      "iteration 1600 / 2000: loss 1.820148\n",
      "iteration 1700 / 2000: loss 1.847259\n",
      "iteration 1800 / 2000: loss 1.862234\n",
      "iteration 1900 / 2000: loss 1.898718\n",
      "iteration 0 / 2000: loss 33.077686\n",
      "iteration 100 / 2000: loss 19.551684\n",
      "iteration 200 / 2000: loss 13.017824\n",
      "iteration 300 / 2000: loss 8.761471\n",
      "iteration 400 / 2000: loss 6.277088\n",
      "iteration 500 / 2000: loss 4.681000\n",
      "iteration 600 / 2000: loss 3.680380\n",
      "iteration 700 / 2000: loss 2.918621\n",
      "iteration 800 / 2000: loss 2.536903\n",
      "iteration 900 / 2000: loss 2.260987\n",
      "iteration 1000 / 2000: loss 2.140879\n",
      "iteration 1100 / 2000: loss 2.125063\n",
      "iteration 1200 / 2000: loss 1.928901\n",
      "iteration 1300 / 2000: loss 2.045643\n",
      "iteration 1400 / 2000: loss 1.870468\n",
      "iteration 1500 / 2000: loss 1.863277\n",
      "iteration 1600 / 2000: loss 1.756249\n",
      "iteration 1700 / 2000: loss 1.776547\n",
      "iteration 1800 / 2000: loss 1.832073\n",
      "iteration 1900 / 2000: loss 1.935424\n",
      "iteration 0 / 2000: loss 39.435717\n",
      "iteration 100 / 2000: loss 21.545564\n",
      "iteration 200 / 2000: loss 13.113853\n",
      "iteration 300 / 2000: loss 8.139748\n",
      "iteration 400 / 2000: loss 5.377919\n",
      "iteration 500 / 2000: loss 3.950316\n",
      "iteration 600 / 2000: loss 3.064746\n",
      "iteration 700 / 2000: loss 2.468286\n",
      "iteration 800 / 2000: loss 2.196622\n",
      "iteration 900 / 2000: loss 2.047473\n",
      "iteration 1000 / 2000: loss 1.923432\n",
      "iteration 1100 / 2000: loss 2.037421\n",
      "iteration 1200 / 2000: loss 1.933503\n",
      "iteration 1300 / 2000: loss 1.873675\n",
      "iteration 1400 / 2000: loss 1.843373\n",
      "iteration 1500 / 2000: loss 1.962860\n",
      "iteration 1600 / 2000: loss 1.832239\n",
      "iteration 1700 / 2000: loss 1.935687\n",
      "iteration 1800 / 2000: loss 1.860472\n",
      "iteration 1900 / 2000: loss 1.831958\n",
      "iteration 0 / 2000: loss 46.872982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: loss 23.122718\n",
      "iteration 200 / 2000: loss 12.406608\n",
      "iteration 300 / 2000: loss 7.277041\n",
      "iteration 400 / 2000: loss 4.638713\n",
      "iteration 500 / 2000: loss 3.261128\n",
      "iteration 600 / 2000: loss 2.558892\n",
      "iteration 700 / 2000: loss 2.248062\n",
      "iteration 800 / 2000: loss 2.040942\n",
      "iteration 900 / 2000: loss 1.860940\n",
      "iteration 1000 / 2000: loss 1.835283\n",
      "iteration 1100 / 2000: loss 2.004551\n",
      "iteration 1200 / 2000: loss 1.884924\n",
      "iteration 1300 / 2000: loss 1.949290\n",
      "iteration 1400 / 2000: loss 1.923421\n",
      "iteration 1500 / 2000: loss 1.918333\n",
      "iteration 1600 / 2000: loss 1.904868\n",
      "iteration 1700 / 2000: loss 1.817791\n",
      "iteration 1800 / 2000: loss 1.853321\n",
      "iteration 1900 / 2000: loss 1.865582\n",
      "iteration 0 / 2000: loss 53.602579\n",
      "iteration 100 / 2000: loss 23.874510\n",
      "iteration 200 / 2000: loss 11.705686\n",
      "iteration 300 / 2000: loss 6.487625\n",
      "iteration 400 / 2000: loss 3.928393\n",
      "iteration 500 / 2000: loss 2.790433\n",
      "iteration 600 / 2000: loss 2.209374\n",
      "iteration 700 / 2000: loss 2.035993\n",
      "iteration 800 / 2000: loss 1.914921\n",
      "iteration 900 / 2000: loss 1.957754\n",
      "iteration 1000 / 2000: loss 1.904152\n",
      "iteration 1100 / 2000: loss 1.867696\n",
      "iteration 1200 / 2000: loss 1.838977\n",
      "iteration 1300 / 2000: loss 1.871906\n",
      "iteration 1400 / 2000: loss 1.981098\n",
      "iteration 1500 / 2000: loss 1.833599\n",
      "iteration 1600 / 2000: loss 1.801123\n",
      "iteration 1700 / 2000: loss 1.845991\n",
      "iteration 1800 / 2000: loss 1.935804\n",
      "iteration 1900 / 2000: loss 1.867247\n",
      "iteration 0 / 2000: loss 61.166582\n",
      "iteration 100 / 2000: loss 24.270110\n",
      "iteration 200 / 2000: loss 11.004941\n",
      "iteration 300 / 2000: loss 5.520365\n",
      "iteration 400 / 2000: loss 3.382808\n",
      "iteration 500 / 2000: loss 2.495793\n",
      "iteration 600 / 2000: loss 2.235635\n",
      "iteration 700 / 2000: loss 1.926208\n",
      "iteration 800 / 2000: loss 1.992206\n",
      "iteration 900 / 2000: loss 1.909131\n",
      "iteration 1000 / 2000: loss 1.825263\n",
      "iteration 1100 / 2000: loss 1.922943\n",
      "iteration 1200 / 2000: loss 1.838231\n",
      "iteration 1300 / 2000: loss 1.916044\n",
      "iteration 1400 / 2000: loss 1.926384\n",
      "iteration 1500 / 2000: loss 1.885767\n",
      "iteration 1600 / 2000: loss 2.025034\n",
      "iteration 1700 / 2000: loss 1.839552\n",
      "iteration 1800 / 2000: loss 1.741912\n",
      "iteration 1900 / 2000: loss 1.886292\n",
      "iteration 0 / 2000: loss 68.291931\n",
      "iteration 100 / 2000: loss 24.581037\n",
      "iteration 200 / 2000: loss 10.088593\n",
      "iteration 300 / 2000: loss 4.915328\n",
      "iteration 400 / 2000: loss 2.946033\n",
      "iteration 500 / 2000: loss 2.382096\n",
      "iteration 600 / 2000: loss 1.989610\n",
      "iteration 700 / 2000: loss 1.946135\n",
      "iteration 800 / 2000: loss 2.003527\n",
      "iteration 900 / 2000: loss 1.879290\n",
      "iteration 1000 / 2000: loss 1.831213\n",
      "iteration 1100 / 2000: loss 1.934747\n",
      "iteration 1200 / 2000: loss 1.846712\n",
      "iteration 1300 / 2000: loss 1.886478\n",
      "iteration 1400 / 2000: loss 1.813230\n",
      "iteration 1500 / 2000: loss 2.034178\n",
      "iteration 1600 / 2000: loss 1.858611\n",
      "iteration 1700 / 2000: loss 1.763162\n",
      "iteration 1800 / 2000: loss 1.849126\n",
      "iteration 1900 / 2000: loss 1.898811\n",
      "iteration 0 / 2000: loss 6.630893\n",
      "iteration 100 / 2000: loss 2.806552\n",
      "iteration 200 / 2000: loss 2.768821\n",
      "iteration 300 / 2000: loss 2.603361\n",
      "iteration 400 / 2000: loss 2.500430\n",
      "iteration 500 / 2000: loss 2.455412\n",
      "iteration 600 / 2000: loss 2.244186\n",
      "iteration 700 / 2000: loss 2.285460\n",
      "iteration 800 / 2000: loss 2.255585\n",
      "iteration 900 / 2000: loss 2.171623\n",
      "iteration 1000 / 2000: loss 2.140814\n",
      "iteration 1100 / 2000: loss 2.177291\n",
      "iteration 1200 / 2000: loss 2.218198\n",
      "iteration 1300 / 2000: loss 2.274459\n",
      "iteration 1400 / 2000: loss 2.117204\n",
      "iteration 1500 / 2000: loss 2.012422\n",
      "iteration 1600 / 2000: loss 2.059084\n",
      "iteration 1700 / 2000: loss 2.030915\n",
      "iteration 1800 / 2000: loss 2.156389\n",
      "iteration 1900 / 2000: loss 2.087033\n",
      "iteration 0 / 2000: loss 12.420642\n",
      "iteration 100 / 2000: loss 8.491777\n",
      "iteration 200 / 2000: loss 7.705364\n",
      "iteration 300 / 2000: loss 6.769068\n",
      "iteration 400 / 2000: loss 5.972058\n",
      "iteration 500 / 2000: loss 5.578743\n",
      "iteration 600 / 2000: loss 4.970827\n",
      "iteration 700 / 2000: loss 4.462952\n",
      "iteration 800 / 2000: loss 4.226290\n",
      "iteration 900 / 2000: loss 3.813253\n",
      "iteration 1000 / 2000: loss 3.612252\n",
      "iteration 1100 / 2000: loss 3.388142\n",
      "iteration 1200 / 2000: loss 3.156400\n",
      "iteration 1300 / 2000: loss 3.202825\n",
      "iteration 1400 / 2000: loss 2.676320\n",
      "iteration 1500 / 2000: loss 2.622872\n",
      "iteration 1600 / 2000: loss 2.621529\n",
      "iteration 1700 / 2000: loss 2.509281\n",
      "iteration 1800 / 2000: loss 2.339883\n",
      "iteration 1900 / 2000: loss 2.200579\n",
      "iteration 0 / 2000: loss 19.764186\n",
      "iteration 100 / 2000: loss 12.503152\n",
      "iteration 200 / 2000: loss 10.146688\n",
      "iteration 300 / 2000: loss 7.940213\n",
      "iteration 400 / 2000: loss 6.646511\n",
      "iteration 500 / 2000: loss 5.557675\n",
      "iteration 600 / 2000: loss 4.657832\n",
      "iteration 700 / 2000: loss 3.955349\n",
      "iteration 800 / 2000: loss 3.556395\n",
      "iteration 900 / 2000: loss 3.003989\n",
      "iteration 1000 / 2000: loss 2.682991\n",
      "iteration 1100 / 2000: loss 2.521370\n",
      "iteration 1200 / 2000: loss 2.361018\n",
      "iteration 1300 / 2000: loss 2.161154\n",
      "iteration 1400 / 2000: loss 2.214627\n",
      "iteration 1500 / 2000: loss 2.050436\n",
      "iteration 1600 / 2000: loss 1.927729\n",
      "iteration 1700 / 2000: loss 1.895322\n",
      "iteration 1800 / 2000: loss 1.871995\n",
      "iteration 1900 / 2000: loss 1.884963\n",
      "iteration 0 / 2000: loss 26.701009\n",
      "iteration 100 / 2000: loss 16.250343\n",
      "iteration 200 / 2000: loss 11.465934\n",
      "iteration 300 / 2000: loss 8.324822\n",
      "iteration 400 / 2000: loss 6.186552\n",
      "iteration 500 / 2000: loss 4.703768\n",
      "iteration 600 / 2000: loss 3.801151\n",
      "iteration 700 / 2000: loss 3.185679\n",
      "iteration 800 / 2000: loss 2.627989\n",
      "iteration 900 / 2000: loss 2.444171\n",
      "iteration 1000 / 2000: loss 2.310885\n",
      "iteration 1100 / 2000: loss 2.016848\n",
      "iteration 1200 / 2000: loss 2.002923\n",
      "iteration 1300 / 2000: loss 1.816670\n",
      "iteration 1400 / 2000: loss 1.811744\n",
      "iteration 1500 / 2000: loss 2.060251\n",
      "iteration 1600 / 2000: loss 1.835713\n",
      "iteration 1700 / 2000: loss 1.841657\n",
      "iteration 1800 / 2000: loss 1.868645\n",
      "iteration 1900 / 2000: loss 1.850865\n",
      "iteration 0 / 2000: loss 34.155721\n",
      "iteration 100 / 2000: loss 18.467013\n",
      "iteration 200 / 2000: loss 11.641047\n",
      "iteration 300 / 2000: loss 7.680915\n",
      "iteration 400 / 2000: loss 5.281193\n",
      "iteration 500 / 2000: loss 3.733947\n",
      "iteration 600 / 2000: loss 3.115851\n",
      "iteration 700 / 2000: loss 2.503644\n",
      "iteration 800 / 2000: loss 2.348591\n",
      "iteration 900 / 2000: loss 2.114779\n",
      "iteration 1000 / 2000: loss 1.905921\n",
      "iteration 1100 / 2000: loss 1.974925\n",
      "iteration 1200 / 2000: loss 1.832977\n",
      "iteration 1300 / 2000: loss 1.888838\n",
      "iteration 1400 / 2000: loss 1.808399\n",
      "iteration 1500 / 2000: loss 1.868500\n",
      "iteration 1600 / 2000: loss 1.780562\n",
      "iteration 1700 / 2000: loss 1.822872\n",
      "iteration 1800 / 2000: loss 1.916788\n",
      "iteration 1900 / 2000: loss 1.823817\n",
      "iteration 0 / 2000: loss 39.310823\n",
      "iteration 100 / 2000: loss 19.666780\n",
      "iteration 200 / 2000: loss 10.936795\n",
      "iteration 300 / 2000: loss 6.601425\n",
      "iteration 400 / 2000: loss 4.278959\n",
      "iteration 500 / 2000: loss 3.119084\n",
      "iteration 600 / 2000: loss 2.574823\n",
      "iteration 700 / 2000: loss 2.206355\n",
      "iteration 800 / 2000: loss 2.031108\n",
      "iteration 900 / 2000: loss 1.885059\n",
      "iteration 1000 / 2000: loss 1.835819\n",
      "iteration 1100 / 2000: loss 1.863776\n",
      "iteration 1200 / 2000: loss 1.795080\n",
      "iteration 1300 / 2000: loss 1.819235\n",
      "iteration 1400 / 2000: loss 1.860497\n",
      "iteration 1500 / 2000: loss 1.799097\n",
      "iteration 1600 / 2000: loss 1.955245\n",
      "iteration 1700 / 2000: loss 1.844068\n",
      "iteration 1800 / 2000: loss 1.859110\n",
      "iteration 1900 / 2000: loss 1.946839\n",
      "iteration 0 / 2000: loss 47.234876\n",
      "iteration 100 / 2000: loss 21.309573\n",
      "iteration 200 / 2000: loss 10.737601\n",
      "iteration 300 / 2000: loss 5.897730\n",
      "iteration 400 / 2000: loss 3.714551\n",
      "iteration 500 / 2000: loss 2.682866\n",
      "iteration 600 / 2000: loss 2.195223\n",
      "iteration 700 / 2000: loss 2.050515\n",
      "iteration 800 / 2000: loss 1.855279\n",
      "iteration 900 / 2000: loss 1.938573\n",
      "iteration 1000 / 2000: loss 1.834077\n",
      "iteration 1100 / 2000: loss 2.098548\n",
      "iteration 1200 / 2000: loss 1.909895\n",
      "iteration 1300 / 2000: loss 1.807861\n",
      "iteration 1400 / 2000: loss 1.851731\n",
      "iteration 1500 / 2000: loss 1.842661\n",
      "iteration 1600 / 2000: loss 1.872167\n",
      "iteration 1700 / 2000: loss 1.864091\n",
      "iteration 1800 / 2000: loss 1.998724\n",
      "iteration 1900 / 2000: loss 1.883991\n",
      "iteration 0 / 2000: loss 53.590431\n",
      "iteration 100 / 2000: loss 21.559053\n",
      "iteration 200 / 2000: loss 9.827281\n",
      "iteration 300 / 2000: loss 5.056681\n",
      "iteration 400 / 2000: loss 3.185463\n",
      "iteration 500 / 2000: loss 2.424099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 2000: loss 2.133844\n",
      "iteration 700 / 2000: loss 1.904525\n",
      "iteration 800 / 2000: loss 1.922808\n",
      "iteration 900 / 2000: loss 1.991188\n",
      "iteration 1000 / 2000: loss 1.964017\n",
      "iteration 1100 / 2000: loss 1.880533\n",
      "iteration 1200 / 2000: loss 1.928202\n",
      "iteration 1300 / 2000: loss 1.863732\n",
      "iteration 1400 / 2000: loss 1.886835\n",
      "iteration 1500 / 2000: loss 1.898641\n",
      "iteration 1600 / 2000: loss 1.810608\n",
      "iteration 1700 / 2000: loss 1.882434\n",
      "iteration 1800 / 2000: loss 1.760422\n",
      "iteration 1900 / 2000: loss 1.866286\n",
      "iteration 0 / 2000: loss 60.445817\n",
      "iteration 100 / 2000: loss 21.502243\n",
      "iteration 200 / 2000: loss 8.860638\n",
      "iteration 300 / 2000: loss 4.281476\n",
      "iteration 400 / 2000: loss 2.840090\n",
      "iteration 500 / 2000: loss 2.198419\n",
      "iteration 600 / 2000: loss 2.024519\n",
      "iteration 700 / 2000: loss 1.914997\n",
      "iteration 800 / 2000: loss 1.867301\n",
      "iteration 900 / 2000: loss 1.837238\n",
      "iteration 1000 / 2000: loss 1.888614\n",
      "iteration 1100 / 2000: loss 1.875274\n",
      "iteration 1200 / 2000: loss 1.976519\n",
      "iteration 1300 / 2000: loss 1.930500\n",
      "iteration 1400 / 2000: loss 1.912537\n",
      "iteration 1500 / 2000: loss 1.810381\n",
      "iteration 1600 / 2000: loss 1.918764\n",
      "iteration 1700 / 2000: loss 1.917999\n",
      "iteration 1800 / 2000: loss 1.868658\n",
      "iteration 1900 / 2000: loss 1.786523\n",
      "iteration 0 / 2000: loss 65.880286\n",
      "iteration 100 / 2000: loss 21.154120\n",
      "iteration 200 / 2000: loss 7.932933\n",
      "iteration 300 / 2000: loss 3.811468\n",
      "iteration 400 / 2000: loss 2.511509\n",
      "iteration 500 / 2000: loss 2.033596\n",
      "iteration 600 / 2000: loss 1.924059\n",
      "iteration 700 / 2000: loss 2.019756\n",
      "iteration 800 / 2000: loss 1.911465\n",
      "iteration 900 / 2000: loss 1.891079\n",
      "iteration 1000 / 2000: loss 1.875777\n",
      "iteration 1100 / 2000: loss 1.800285\n",
      "iteration 1200 / 2000: loss 1.870663\n",
      "iteration 1300 / 2000: loss 1.899254\n",
      "iteration 1400 / 2000: loss 1.813868\n",
      "iteration 1500 / 2000: loss 1.897158\n",
      "iteration 1600 / 2000: loss 1.944664\n",
      "iteration 1700 / 2000: loss 1.918875\n",
      "iteration 1800 / 2000: loss 1.854589\n",
      "iteration 1900 / 2000: loss 1.782124\n",
      "iteration 0 / 2000: loss 6.888844\n",
      "iteration 100 / 2000: loss 3.139673\n",
      "iteration 200 / 2000: loss 2.646259\n",
      "iteration 300 / 2000: loss 2.620104\n",
      "iteration 400 / 2000: loss 2.381013\n",
      "iteration 500 / 2000: loss 2.339755\n",
      "iteration 600 / 2000: loss 2.257817\n",
      "iteration 700 / 2000: loss 2.345055\n",
      "iteration 800 / 2000: loss 2.138894\n",
      "iteration 900 / 2000: loss 2.210191\n",
      "iteration 1000 / 2000: loss 2.431167\n",
      "iteration 1100 / 2000: loss 2.006202\n",
      "iteration 1200 / 2000: loss 2.114596\n",
      "iteration 1300 / 2000: loss 2.135300\n",
      "iteration 1400 / 2000: loss 2.306804\n",
      "iteration 1500 / 2000: loss 2.174537\n",
      "iteration 1600 / 2000: loss 2.206676\n",
      "iteration 1700 / 2000: loss 2.014118\n",
      "iteration 1800 / 2000: loss 2.132111\n",
      "iteration 1900 / 2000: loss 2.134537\n",
      "iteration 0 / 2000: loss 12.702648\n",
      "iteration 100 / 2000: loss 8.358489\n",
      "iteration 200 / 2000: loss 7.215819\n",
      "iteration 300 / 2000: loss 6.388385\n",
      "iteration 400 / 2000: loss 5.835847\n",
      "iteration 500 / 2000: loss 4.926453\n",
      "iteration 600 / 2000: loss 4.554550\n",
      "iteration 700 / 2000: loss 4.109211\n",
      "iteration 800 / 2000: loss 3.806470\n",
      "iteration 900 / 2000: loss 3.470812\n",
      "iteration 1000 / 2000: loss 3.160599\n",
      "iteration 1100 / 2000: loss 2.903097\n",
      "iteration 1200 / 2000: loss 2.827729\n",
      "iteration 1300 / 2000: loss 2.578021\n",
      "iteration 1400 / 2000: loss 2.608979\n",
      "iteration 1500 / 2000: loss 2.437942\n",
      "iteration 1600 / 2000: loss 2.318575\n",
      "iteration 1700 / 2000: loss 2.420123\n",
      "iteration 1800 / 2000: loss 2.168299\n",
      "iteration 1900 / 2000: loss 2.212261\n",
      "iteration 0 / 2000: loss 19.785429\n",
      "iteration 100 / 2000: loss 12.797224\n",
      "iteration 200 / 2000: loss 9.681830\n",
      "iteration 300 / 2000: loss 7.514717\n",
      "iteration 400 / 2000: loss 5.988853\n",
      "iteration 500 / 2000: loss 4.793154\n",
      "iteration 600 / 2000: loss 3.964812\n",
      "iteration 700 / 2000: loss 3.457212\n",
      "iteration 800 / 2000: loss 3.112781\n",
      "iteration 900 / 2000: loss 2.784494\n",
      "iteration 1000 / 2000: loss 2.516650\n",
      "iteration 1100 / 2000: loss 2.361596\n",
      "iteration 1200 / 2000: loss 2.183127\n",
      "iteration 1300 / 2000: loss 1.951561\n",
      "iteration 1400 / 2000: loss 1.966349\n",
      "iteration 1500 / 2000: loss 1.936438\n",
      "iteration 1600 / 2000: loss 2.019906\n",
      "iteration 1700 / 2000: loss 1.919306\n",
      "iteration 1800 / 2000: loss 1.836712\n",
      "iteration 1900 / 2000: loss 1.854053\n",
      "iteration 0 / 2000: loss 25.969298\n",
      "iteration 100 / 2000: loss 15.247528\n",
      "iteration 200 / 2000: loss 10.373131\n",
      "iteration 300 / 2000: loss 7.304638\n",
      "iteration 400 / 2000: loss 5.271989\n",
      "iteration 500 / 2000: loss 4.086927\n",
      "iteration 600 / 2000: loss 3.255371\n",
      "iteration 700 / 2000: loss 2.712502\n",
      "iteration 800 / 2000: loss 2.318933\n",
      "iteration 900 / 2000: loss 2.200411\n",
      "iteration 1000 / 2000: loss 1.923137\n",
      "iteration 1100 / 2000: loss 1.952888\n",
      "iteration 1200 / 2000: loss 1.978602\n",
      "iteration 1300 / 2000: loss 1.830339\n",
      "iteration 1400 / 2000: loss 1.831905\n",
      "iteration 1500 / 2000: loss 1.890479\n",
      "iteration 1600 / 2000: loss 1.675290\n",
      "iteration 1700 / 2000: loss 1.711783\n",
      "iteration 1800 / 2000: loss 1.886138\n",
      "iteration 1900 / 2000: loss 1.789026\n",
      "iteration 0 / 2000: loss 32.387331\n",
      "iteration 100 / 2000: loss 17.170140\n",
      "iteration 200 / 2000: loss 10.177354\n",
      "iteration 300 / 2000: loss 6.541272\n",
      "iteration 400 / 2000: loss 4.411561\n",
      "iteration 500 / 2000: loss 3.304087\n",
      "iteration 600 / 2000: loss 2.557926\n",
      "iteration 700 / 2000: loss 2.330438\n",
      "iteration 800 / 2000: loss 2.035168\n",
      "iteration 900 / 2000: loss 1.874389\n",
      "iteration 1000 / 2000: loss 1.886233\n",
      "iteration 1100 / 2000: loss 1.915120\n",
      "iteration 1200 / 2000: loss 1.968709\n",
      "iteration 1300 / 2000: loss 1.952489\n",
      "iteration 1400 / 2000: loss 1.850596\n",
      "iteration 1500 / 2000: loss 1.887281\n",
      "iteration 1600 / 2000: loss 1.843940\n",
      "iteration 1700 / 2000: loss 1.717192\n",
      "iteration 1800 / 2000: loss 1.817806\n",
      "iteration 1900 / 2000: loss 1.898127\n",
      "iteration 0 / 2000: loss 39.536909\n",
      "iteration 100 / 2000: loss 18.143264\n",
      "iteration 200 / 2000: loss 9.659935\n",
      "iteration 300 / 2000: loss 5.613383\n",
      "iteration 400 / 2000: loss 3.625073\n",
      "iteration 500 / 2000: loss 2.773742\n",
      "iteration 600 / 2000: loss 2.275763\n",
      "iteration 700 / 2000: loss 2.138620\n",
      "iteration 800 / 2000: loss 1.886251\n",
      "iteration 900 / 2000: loss 1.959413\n",
      "iteration 1000 / 2000: loss 1.930492\n",
      "iteration 1100 / 2000: loss 1.931839\n",
      "iteration 1200 / 2000: loss 1.839104\n",
      "iteration 1300 / 2000: loss 1.877872\n",
      "iteration 1400 / 2000: loss 1.797373\n",
      "iteration 1500 / 2000: loss 1.856751\n",
      "iteration 1600 / 2000: loss 1.865529\n",
      "iteration 1700 / 2000: loss 1.789820\n",
      "iteration 1800 / 2000: loss 1.862563\n",
      "iteration 1900 / 2000: loss 1.866502\n",
      "iteration 0 / 2000: loss 46.889446\n",
      "iteration 100 / 2000: loss 19.071335\n",
      "iteration 200 / 2000: loss 8.964005\n",
      "iteration 300 / 2000: loss 4.812931\n",
      "iteration 400 / 2000: loss 3.056484\n",
      "iteration 500 / 2000: loss 2.328911\n",
      "iteration 600 / 2000: loss 2.024864\n",
      "iteration 700 / 2000: loss 1.979568\n",
      "iteration 800 / 2000: loss 1.727501\n",
      "iteration 900 / 2000: loss 1.908742\n",
      "iteration 1000 / 2000: loss 1.848795\n",
      "iteration 1100 / 2000: loss 1.843134\n",
      "iteration 1200 / 2000: loss 1.931877\n",
      "iteration 1300 / 2000: loss 1.772520\n",
      "iteration 1400 / 2000: loss 1.871008\n",
      "iteration 1500 / 2000: loss 1.799622\n",
      "iteration 1600 / 2000: loss 1.970491\n",
      "iteration 1700 / 2000: loss 1.916886\n",
      "iteration 1800 / 2000: loss 1.840140\n",
      "iteration 1900 / 2000: loss 1.876546\n",
      "iteration 0 / 2000: loss 52.877915\n",
      "iteration 100 / 2000: loss 19.201248\n",
      "iteration 200 / 2000: loss 8.061344\n",
      "iteration 300 / 2000: loss 4.035561\n",
      "iteration 400 / 2000: loss 2.667315\n",
      "iteration 500 / 2000: loss 2.136756\n",
      "iteration 600 / 2000: loss 1.924557\n",
      "iteration 700 / 2000: loss 1.781248\n",
      "iteration 800 / 2000: loss 1.936522\n",
      "iteration 900 / 2000: loss 1.925009\n",
      "iteration 1000 / 2000: loss 1.952863\n",
      "iteration 1100 / 2000: loss 1.834713\n",
      "iteration 1200 / 2000: loss 1.927254\n",
      "iteration 1300 / 2000: loss 1.904421\n",
      "iteration 1400 / 2000: loss 1.972484\n",
      "iteration 1500 / 2000: loss 1.804124\n",
      "iteration 1600 / 2000: loss 2.068943\n",
      "iteration 1700 / 2000: loss 1.809028\n",
      "iteration 1800 / 2000: loss 1.849188\n",
      "iteration 1900 / 2000: loss 1.871592\n",
      "iteration 0 / 2000: loss 60.628310\n",
      "iteration 100 / 2000: loss 19.156960\n",
      "iteration 200 / 2000: loss 7.218104\n",
      "iteration 300 / 2000: loss 3.580030\n",
      "iteration 400 / 2000: loss 2.467165\n",
      "iteration 500 / 2000: loss 2.090404\n",
      "iteration 600 / 2000: loss 1.978987\n",
      "iteration 700 / 2000: loss 1.923654\n",
      "iteration 800 / 2000: loss 1.858683\n",
      "iteration 900 / 2000: loss 1.900249\n",
      "iteration 1000 / 2000: loss 1.794481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 2000: loss 1.942213\n",
      "iteration 1200 / 2000: loss 1.981144\n",
      "iteration 1300 / 2000: loss 1.929888\n",
      "iteration 1400 / 2000: loss 1.996661\n",
      "iteration 1500 / 2000: loss 1.818336\n",
      "iteration 1600 / 2000: loss 1.875279\n",
      "iteration 1700 / 2000: loss 1.792015\n",
      "iteration 1800 / 2000: loss 1.897810\n",
      "iteration 1900 / 2000: loss 1.968229\n",
      "iteration 0 / 2000: loss 67.557968\n",
      "iteration 100 / 2000: loss 18.596132\n",
      "iteration 200 / 2000: loss 6.151491\n",
      "iteration 300 / 2000: loss 3.108074\n",
      "iteration 400 / 2000: loss 2.163228\n",
      "iteration 500 / 2000: loss 1.964810\n",
      "iteration 600 / 2000: loss 1.921514\n",
      "iteration 700 / 2000: loss 1.845892\n",
      "iteration 800 / 2000: loss 1.864284\n",
      "iteration 900 / 2000: loss 1.861819\n",
      "iteration 1000 / 2000: loss 1.895095\n",
      "iteration 1100 / 2000: loss 1.917376\n",
      "iteration 1200 / 2000: loss 1.956814\n",
      "iteration 1300 / 2000: loss 1.940232\n",
      "iteration 1400 / 2000: loss 1.880812\n",
      "iteration 1500 / 2000: loss 1.926278\n",
      "iteration 1600 / 2000: loss 1.826805\n",
      "iteration 1700 / 2000: loss 1.851556\n",
      "iteration 1800 / 2000: loss 1.921797\n",
      "iteration 1900 / 2000: loss 1.984580\n",
      "iteration 0 / 2000: loss 5.596372\n",
      "iteration 100 / 2000: loss 2.841683\n",
      "iteration 200 / 2000: loss 2.446450\n",
      "iteration 300 / 2000: loss 2.354721\n",
      "iteration 400 / 2000: loss 2.605784\n",
      "iteration 500 / 2000: loss 2.399574\n",
      "iteration 600 / 2000: loss 2.220585\n",
      "iteration 700 / 2000: loss 2.232399\n",
      "iteration 800 / 2000: loss 2.306975\n",
      "iteration 900 / 2000: loss 2.156424\n",
      "iteration 1000 / 2000: loss 2.147625\n",
      "iteration 1100 / 2000: loss 2.128208\n",
      "iteration 1200 / 2000: loss 2.286096\n",
      "iteration 1300 / 2000: loss 2.243610\n",
      "iteration 1400 / 2000: loss 2.109656\n",
      "iteration 1500 / 2000: loss 2.209784\n",
      "iteration 1600 / 2000: loss 2.058574\n",
      "iteration 1700 / 2000: loss 2.157347\n",
      "iteration 1800 / 2000: loss 1.951782\n",
      "iteration 1900 / 2000: loss 2.019307\n",
      "iteration 0 / 2000: loss 12.424633\n",
      "iteration 100 / 2000: loss 8.441229\n",
      "iteration 200 / 2000: loss 7.267512\n",
      "iteration 300 / 2000: loss 6.345501\n",
      "iteration 400 / 2000: loss 5.474356\n",
      "iteration 500 / 2000: loss 4.817988\n",
      "iteration 600 / 2000: loss 4.324867\n",
      "iteration 700 / 2000: loss 3.935345\n",
      "iteration 800 / 2000: loss 3.416403\n",
      "iteration 900 / 2000: loss 3.356765\n",
      "iteration 1000 / 2000: loss 2.983843\n",
      "iteration 1100 / 2000: loss 2.781995\n",
      "iteration 1200 / 2000: loss 2.564904\n",
      "iteration 1300 / 2000: loss 2.408322\n",
      "iteration 1400 / 2000: loss 2.378503\n",
      "iteration 1500 / 2000: loss 2.140575\n",
      "iteration 1600 / 2000: loss 2.347967\n",
      "iteration 1700 / 2000: loss 2.113052\n",
      "iteration 1800 / 2000: loss 2.015611\n",
      "iteration 1900 / 2000: loss 1.920451\n",
      "iteration 0 / 2000: loss 19.171517\n",
      "iteration 100 / 2000: loss 12.153532\n",
      "iteration 200 / 2000: loss 9.037069\n",
      "iteration 300 / 2000: loss 6.820312\n",
      "iteration 400 / 2000: loss 5.313131\n",
      "iteration 500 / 2000: loss 4.360216\n",
      "iteration 600 / 2000: loss 3.648773\n",
      "iteration 700 / 2000: loss 3.123756\n",
      "iteration 800 / 2000: loss 2.730264\n",
      "iteration 900 / 2000: loss 2.473385\n",
      "iteration 1000 / 2000: loss 2.384291\n",
      "iteration 1100 / 2000: loss 2.236189\n",
      "iteration 1200 / 2000: loss 2.097631\n",
      "iteration 1300 / 2000: loss 1.962853\n",
      "iteration 1400 / 2000: loss 1.808428\n",
      "iteration 1500 / 2000: loss 1.998089\n",
      "iteration 1600 / 2000: loss 1.836199\n",
      "iteration 1700 / 2000: loss 1.754157\n",
      "iteration 1800 / 2000: loss 1.844235\n",
      "iteration 1900 / 2000: loss 1.893858\n",
      "iteration 0 / 2000: loss 25.268141\n",
      "iteration 100 / 2000: loss 14.743022\n",
      "iteration 200 / 2000: loss 9.302031\n",
      "iteration 300 / 2000: loss 6.502817\n",
      "iteration 400 / 2000: loss 4.569675\n",
      "iteration 500 / 2000: loss 3.447573\n",
      "iteration 600 / 2000: loss 2.792925\n",
      "iteration 700 / 2000: loss 2.514404\n",
      "iteration 800 / 2000: loss 2.209041\n",
      "iteration 900 / 2000: loss 1.991680\n",
      "iteration 1000 / 2000: loss 1.937793\n",
      "iteration 1100 / 2000: loss 1.941240\n",
      "iteration 1200 / 2000: loss 1.867838\n",
      "iteration 1300 / 2000: loss 1.845120\n",
      "iteration 1400 / 2000: loss 1.930766\n",
      "iteration 1500 / 2000: loss 1.748642\n",
      "iteration 1600 / 2000: loss 1.832573\n",
      "iteration 1700 / 2000: loss 1.933117\n",
      "iteration 1800 / 2000: loss 1.860922\n",
      "iteration 1900 / 2000: loss 1.784318\n",
      "iteration 0 / 2000: loss 34.121538\n",
      "iteration 100 / 2000: loss 16.443556\n",
      "iteration 200 / 2000: loss 9.407498\n",
      "iteration 300 / 2000: loss 5.800262\n",
      "iteration 400 / 2000: loss 3.800107\n",
      "iteration 500 / 2000: loss 2.829898\n",
      "iteration 600 / 2000: loss 2.262085\n",
      "iteration 700 / 2000: loss 2.057197\n",
      "iteration 800 / 2000: loss 2.057133\n",
      "iteration 900 / 2000: loss 1.911649\n",
      "iteration 1000 / 2000: loss 1.788839\n",
      "iteration 1100 / 2000: loss 1.863568\n",
      "iteration 1200 / 2000: loss 1.891363\n",
      "iteration 1300 / 2000: loss 1.896438\n",
      "iteration 1400 / 2000: loss 1.691398\n",
      "iteration 1500 / 2000: loss 1.829605\n",
      "iteration 1600 / 2000: loss 1.806070\n",
      "iteration 1700 / 2000: loss 1.822108\n",
      "iteration 1800 / 2000: loss 1.795301\n",
      "iteration 1900 / 2000: loss 1.821604\n",
      "iteration 0 / 2000: loss 40.852444\n",
      "iteration 100 / 2000: loss 17.130936\n",
      "iteration 200 / 2000: loss 8.476429\n",
      "iteration 300 / 2000: loss 4.696622\n",
      "iteration 400 / 2000: loss 3.088521\n",
      "iteration 500 / 2000: loss 2.476992\n",
      "iteration 600 / 2000: loss 2.034972\n",
      "iteration 700 / 2000: loss 1.914427\n",
      "iteration 800 / 2000: loss 1.906869\n",
      "iteration 900 / 2000: loss 1.907582\n",
      "iteration 1000 / 2000: loss 1.852791\n",
      "iteration 1100 / 2000: loss 1.807349\n",
      "iteration 1200 / 2000: loss 1.912613\n",
      "iteration 1300 / 2000: loss 1.802128\n",
      "iteration 1400 / 2000: loss 1.961604\n",
      "iteration 1500 / 2000: loss 1.869642\n",
      "iteration 1600 / 2000: loss 1.808572\n",
      "iteration 1700 / 2000: loss 1.771396\n",
      "iteration 1800 / 2000: loss 1.867745\n",
      "iteration 1900 / 2000: loss 1.890241\n",
      "iteration 0 / 2000: loss 46.835294\n",
      "iteration 100 / 2000: loss 17.460472\n",
      "iteration 200 / 2000: loss 7.582400\n",
      "iteration 300 / 2000: loss 3.989573\n",
      "iteration 400 / 2000: loss 2.768929\n",
      "iteration 500 / 2000: loss 2.300070\n",
      "iteration 600 / 2000: loss 1.940226\n",
      "iteration 700 / 2000: loss 1.891646\n",
      "iteration 800 / 2000: loss 1.996709\n",
      "iteration 900 / 2000: loss 1.931822\n",
      "iteration 1000 / 2000: loss 1.910533\n",
      "iteration 1100 / 2000: loss 1.916065\n",
      "iteration 1200 / 2000: loss 1.862426\n",
      "iteration 1300 / 2000: loss 1.788040\n",
      "iteration 1400 / 2000: loss 1.781029\n",
      "iteration 1500 / 2000: loss 1.921097\n",
      "iteration 1600 / 2000: loss 1.961604\n",
      "iteration 1700 / 2000: loss 1.835545\n",
      "iteration 1800 / 2000: loss 2.056711\n",
      "iteration 1900 / 2000: loss 1.882092\n",
      "iteration 0 / 2000: loss 53.424998\n",
      "iteration 100 / 2000: loss 17.289263\n",
      "iteration 200 / 2000: loss 6.770828\n",
      "iteration 300 / 2000: loss 3.383546\n",
      "iteration 400 / 2000: loss 2.402759\n",
      "iteration 500 / 2000: loss 2.014006\n",
      "iteration 600 / 2000: loss 1.881547\n",
      "iteration 700 / 2000: loss 1.964383\n",
      "iteration 800 / 2000: loss 1.853429\n",
      "iteration 900 / 2000: loss 1.858105\n",
      "iteration 1000 / 2000: loss 1.907884\n",
      "iteration 1100 / 2000: loss 1.829858\n",
      "iteration 1200 / 2000: loss 1.802188\n",
      "iteration 1300 / 2000: loss 1.829023\n",
      "iteration 1400 / 2000: loss 1.923864\n",
      "iteration 1500 / 2000: loss 1.773457\n",
      "iteration 1600 / 2000: loss 1.866032\n",
      "iteration 1700 / 2000: loss 1.922665\n",
      "iteration 1800 / 2000: loss 1.866924\n",
      "iteration 1900 / 2000: loss 1.856639\n",
      "iteration 0 / 2000: loss 60.368299\n",
      "iteration 100 / 2000: loss 16.854715\n",
      "iteration 200 / 2000: loss 5.879304\n",
      "iteration 300 / 2000: loss 3.029764\n",
      "iteration 400 / 2000: loss 2.094771\n",
      "iteration 500 / 2000: loss 1.999831\n",
      "iteration 600 / 2000: loss 2.021477\n",
      "iteration 700 / 2000: loss 1.950245\n",
      "iteration 800 / 2000: loss 1.825008\n",
      "iteration 900 / 2000: loss 1.906799\n",
      "iteration 1000 / 2000: loss 1.913507\n",
      "iteration 1100 / 2000: loss 1.945761\n",
      "iteration 1200 / 2000: loss 1.866468\n",
      "iteration 1300 / 2000: loss 1.804138\n",
      "iteration 1400 / 2000: loss 1.808283\n",
      "iteration 1500 / 2000: loss 1.875186\n",
      "iteration 1600 / 2000: loss 1.969028\n",
      "iteration 1700 / 2000: loss 1.871613\n",
      "iteration 1800 / 2000: loss 1.897896\n",
      "iteration 1900 / 2000: loss 1.880477\n",
      "iteration 0 / 2000: loss 67.999006\n",
      "iteration 100 / 2000: loss 16.220634\n",
      "iteration 200 / 2000: loss 5.161075\n",
      "iteration 300 / 2000: loss 2.707467\n",
      "iteration 400 / 2000: loss 2.128712\n",
      "iteration 500 / 2000: loss 1.977897\n",
      "iteration 600 / 2000: loss 2.003327\n",
      "iteration 700 / 2000: loss 1.875849\n",
      "iteration 800 / 2000: loss 2.001489\n",
      "iteration 900 / 2000: loss 1.852620\n",
      "iteration 1000 / 2000: loss 1.839906\n",
      "iteration 1100 / 2000: loss 1.849274\n",
      "iteration 1200 / 2000: loss 1.975587\n",
      "iteration 1300 / 2000: loss 1.920633\n",
      "iteration 1400 / 2000: loss 1.880630\n",
      "iteration 1500 / 2000: loss 1.985837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600 / 2000: loss 1.940068\n",
      "iteration 1700 / 2000: loss 2.018796\n",
      "iteration 1800 / 2000: loss 1.852861\n",
      "iteration 1900 / 2000: loss 1.879451\n",
      "iteration 0 / 2000: loss 5.704468\n",
      "iteration 100 / 2000: loss 2.899752\n",
      "iteration 200 / 2000: loss 2.612920\n",
      "iteration 300 / 2000: loss 2.440472\n",
      "iteration 400 / 2000: loss 2.415428\n",
      "iteration 500 / 2000: loss 2.237491\n",
      "iteration 600 / 2000: loss 2.228027\n",
      "iteration 700 / 2000: loss 2.185855\n",
      "iteration 800 / 2000: loss 2.200766\n",
      "iteration 900 / 2000: loss 2.027757\n",
      "iteration 1000 / 2000: loss 2.184540\n",
      "iteration 1100 / 2000: loss 2.123079\n",
      "iteration 1200 / 2000: loss 2.232052\n",
      "iteration 1300 / 2000: loss 2.026025\n",
      "iteration 1400 / 2000: loss 2.069463\n",
      "iteration 1500 / 2000: loss 2.203550\n",
      "iteration 1600 / 2000: loss 1.990263\n",
      "iteration 1700 / 2000: loss 2.088135\n",
      "iteration 1800 / 2000: loss 2.106206\n",
      "iteration 1900 / 2000: loss 1.881854\n",
      "iteration 0 / 2000: loss 13.824365\n",
      "iteration 100 / 2000: loss 8.266277\n",
      "iteration 200 / 2000: loss 6.991428\n",
      "iteration 300 / 2000: loss 5.983503\n",
      "iteration 400 / 2000: loss 5.197698\n",
      "iteration 500 / 2000: loss 4.558941\n",
      "iteration 600 / 2000: loss 4.180715\n",
      "iteration 700 / 2000: loss 3.631923\n",
      "iteration 800 / 2000: loss 3.308120\n",
      "iteration 900 / 2000: loss 3.160385\n",
      "iteration 1000 / 2000: loss 2.786871\n",
      "iteration 1100 / 2000: loss 2.566561\n",
      "iteration 1200 / 2000: loss 2.529951\n",
      "iteration 1300 / 2000: loss 2.356897\n",
      "iteration 1400 / 2000: loss 2.118640\n",
      "iteration 1500 / 2000: loss 2.277314\n",
      "iteration 1600 / 2000: loss 2.123408\n",
      "iteration 1700 / 2000: loss 1.988808\n",
      "iteration 1800 / 2000: loss 1.916625\n",
      "iteration 1900 / 2000: loss 1.954372\n",
      "iteration 0 / 2000: loss 20.344873\n",
      "iteration 100 / 2000: loss 11.746610\n",
      "iteration 200 / 2000: loss 8.540210\n",
      "iteration 300 / 2000: loss 6.342365\n",
      "iteration 400 / 2000: loss 4.954316\n",
      "iteration 500 / 2000: loss 3.918857\n",
      "iteration 600 / 2000: loss 3.219477\n",
      "iteration 700 / 2000: loss 2.833747\n",
      "iteration 800 / 2000: loss 2.404630\n",
      "iteration 900 / 2000: loss 2.357201\n",
      "iteration 1000 / 2000: loss 2.178785\n",
      "iteration 1100 / 2000: loss 2.068296\n",
      "iteration 1200 / 2000: loss 1.943420\n",
      "iteration 1300 / 2000: loss 1.803385\n",
      "iteration 1400 / 2000: loss 1.932770\n",
      "iteration 1500 / 2000: loss 2.008459\n",
      "iteration 1600 / 2000: loss 1.850422\n",
      "iteration 1700 / 2000: loss 1.810373\n",
      "iteration 1800 / 2000: loss 1.787358\n",
      "iteration 1900 / 2000: loss 1.875522\n",
      "iteration 0 / 2000: loss 26.095627\n",
      "iteration 100 / 2000: loss 13.995701\n",
      "iteration 200 / 2000: loss 8.704338\n",
      "iteration 300 / 2000: loss 5.816143\n",
      "iteration 400 / 2000: loss 4.128672\n",
      "iteration 500 / 2000: loss 3.216609\n",
      "iteration 600 / 2000: loss 2.521517\n",
      "iteration 700 / 2000: loss 2.218571\n",
      "iteration 800 / 2000: loss 2.004289\n",
      "iteration 900 / 2000: loss 2.086556\n",
      "iteration 1000 / 2000: loss 2.000252\n",
      "iteration 1100 / 2000: loss 1.803576\n",
      "iteration 1200 / 2000: loss 1.729473\n",
      "iteration 1300 / 2000: loss 1.749869\n",
      "iteration 1400 / 2000: loss 1.792923\n",
      "iteration 1500 / 2000: loss 1.794871\n",
      "iteration 1600 / 2000: loss 1.956803\n",
      "iteration 1700 / 2000: loss 1.773814\n",
      "iteration 1800 / 2000: loss 1.794965\n",
      "iteration 1900 / 2000: loss 1.789317\n",
      "iteration 0 / 2000: loss 31.906869\n",
      "iteration 100 / 2000: loss 14.921555\n",
      "iteration 200 / 2000: loss 8.065262\n",
      "iteration 300 / 2000: loss 4.815945\n",
      "iteration 400 / 2000: loss 3.262547\n",
      "iteration 500 / 2000: loss 2.556710\n",
      "iteration 600 / 2000: loss 2.200824\n",
      "iteration 700 / 2000: loss 1.993848\n",
      "iteration 800 / 2000: loss 1.897648\n",
      "iteration 900 / 2000: loss 1.843498\n",
      "iteration 1000 / 2000: loss 1.817296\n",
      "iteration 1100 / 2000: loss 1.749841\n",
      "iteration 1200 / 2000: loss 1.847590\n",
      "iteration 1300 / 2000: loss 1.926076\n",
      "iteration 1400 / 2000: loss 1.805556\n",
      "iteration 1500 / 2000: loss 1.908540\n",
      "iteration 1600 / 2000: loss 1.869496\n",
      "iteration 1700 / 2000: loss 1.856406\n",
      "iteration 1800 / 2000: loss 1.865459\n",
      "iteration 1900 / 2000: loss 1.903505\n",
      "iteration 0 / 2000: loss 40.494699\n",
      "iteration 100 / 2000: loss 15.778990\n",
      "iteration 200 / 2000: loss 7.453456\n",
      "iteration 300 / 2000: loss 4.308460\n",
      "iteration 400 / 2000: loss 2.874687\n",
      "iteration 500 / 2000: loss 2.356607\n",
      "iteration 600 / 2000: loss 2.019841\n",
      "iteration 700 / 2000: loss 1.944145\n",
      "iteration 800 / 2000: loss 2.038230\n",
      "iteration 900 / 2000: loss 1.794320\n",
      "iteration 1000 / 2000: loss 1.832142\n",
      "iteration 1100 / 2000: loss 1.869584\n",
      "iteration 1200 / 2000: loss 1.836203\n",
      "iteration 1300 / 2000: loss 1.936640\n",
      "iteration 1400 / 2000: loss 1.900354\n",
      "iteration 1500 / 2000: loss 1.793654\n",
      "iteration 1600 / 2000: loss 1.835282\n",
      "iteration 1700 / 2000: loss 1.904543\n",
      "iteration 1800 / 2000: loss 1.812635\n",
      "iteration 1900 / 2000: loss 1.825940\n",
      "iteration 0 / 2000: loss 46.233270\n",
      "iteration 100 / 2000: loss 15.871483\n",
      "iteration 200 / 2000: loss 6.492306\n",
      "iteration 300 / 2000: loss 3.509883\n",
      "iteration 400 / 2000: loss 2.472185\n",
      "iteration 500 / 2000: loss 2.064990\n",
      "iteration 600 / 2000: loss 2.002347\n",
      "iteration 700 / 2000: loss 1.906559\n",
      "iteration 800 / 2000: loss 1.859102\n",
      "iteration 900 / 2000: loss 1.870780\n",
      "iteration 1000 / 2000: loss 1.901350\n",
      "iteration 1100 / 2000: loss 1.977647\n",
      "iteration 1200 / 2000: loss 1.803293\n",
      "iteration 1300 / 2000: loss 1.808296\n",
      "iteration 1400 / 2000: loss 1.794993\n",
      "iteration 1500 / 2000: loss 1.785570\n",
      "iteration 1600 / 2000: loss 1.870635\n",
      "iteration 1700 / 2000: loss 1.835239\n",
      "iteration 1800 / 2000: loss 1.983315\n",
      "iteration 1900 / 2000: loss 1.881190\n",
      "iteration 0 / 2000: loss 52.700765\n",
      "iteration 100 / 2000: loss 15.327585\n",
      "iteration 200 / 2000: loss 5.618586\n",
      "iteration 300 / 2000: loss 2.900349\n",
      "iteration 400 / 2000: loss 2.183162\n",
      "iteration 500 / 2000: loss 1.952755\n",
      "iteration 600 / 2000: loss 1.986938\n",
      "iteration 700 / 2000: loss 1.791329\n",
      "iteration 800 / 2000: loss 1.885166\n",
      "iteration 900 / 2000: loss 1.967586\n",
      "iteration 1000 / 2000: loss 1.845833\n",
      "iteration 1100 / 2000: loss 1.969281\n",
      "iteration 1200 / 2000: loss 1.771873\n",
      "iteration 1300 / 2000: loss 1.837704\n",
      "iteration 1400 / 2000: loss 1.783812\n",
      "iteration 1500 / 2000: loss 1.730310\n",
      "iteration 1600 / 2000: loss 1.850578\n",
      "iteration 1700 / 2000: loss 1.841657\n",
      "iteration 1800 / 2000: loss 1.881538\n",
      "iteration 1900 / 2000: loss 1.856815\n",
      "iteration 0 / 2000: loss 60.315440\n",
      "iteration 100 / 2000: loss 14.810776\n",
      "iteration 200 / 2000: loss 4.965271\n",
      "iteration 300 / 2000: loss 2.582165\n",
      "iteration 400 / 2000: loss 2.037515\n",
      "iteration 500 / 2000: loss 1.951821\n",
      "iteration 600 / 2000: loss 1.811824\n",
      "iteration 700 / 2000: loss 1.911280\n",
      "iteration 800 / 2000: loss 1.987660\n",
      "iteration 900 / 2000: loss 1.825816\n",
      "iteration 1000 / 2000: loss 1.899571\n",
      "iteration 1100 / 2000: loss 2.085142\n",
      "iteration 1200 / 2000: loss 1.954657\n",
      "iteration 1300 / 2000: loss 1.876171\n",
      "iteration 1400 / 2000: loss 1.890888\n",
      "iteration 1500 / 2000: loss 1.829807\n",
      "iteration 1600 / 2000: loss 1.835949\n",
      "iteration 1700 / 2000: loss 1.968547\n",
      "iteration 1800 / 2000: loss 1.912756\n",
      "iteration 1900 / 2000: loss 1.886742\n",
      "iteration 0 / 2000: loss 68.576046\n",
      "iteration 100 / 2000: loss 14.192201\n",
      "iteration 200 / 2000: loss 4.166093\n",
      "iteration 300 / 2000: loss 2.294409\n",
      "iteration 400 / 2000: loss 1.931611\n",
      "iteration 500 / 2000: loss 1.962049\n",
      "iteration 600 / 2000: loss 2.033430\n",
      "iteration 700 / 2000: loss 1.846655\n",
      "iteration 800 / 2000: loss 1.825880\n",
      "iteration 900 / 2000: loss 1.898414\n",
      "iteration 1000 / 2000: loss 1.873242\n",
      "iteration 1100 / 2000: loss 2.035804\n",
      "iteration 1200 / 2000: loss 1.924352\n",
      "iteration 1300 / 2000: loss 1.855834\n",
      "iteration 1400 / 2000: loss 1.900187\n",
      "iteration 1500 / 2000: loss 1.839765\n",
      "iteration 1600 / 2000: loss 1.796476\n",
      "iteration 1700 / 2000: loss 1.849835\n",
      "iteration 1800 / 2000: loss 1.842548\n",
      "iteration 1900 / 2000: loss 1.858233\n",
      "lr 3.000000e-07 reg 1.000000e+01 train accuracy: 0.311327 val accuracy: 0.306000\n",
      "lr 3.000000e-07 reg 2.311111e+02 train accuracy: 0.327102 val accuracy: 0.341000\n",
      "lr 3.000000e-07 reg 4.522222e+02 train accuracy: 0.345755 val accuracy: 0.350000\n",
      "lr 3.000000e-07 reg 6.733333e+02 train accuracy: 0.357918 val accuracy: 0.370000\n",
      "lr 3.000000e-07 reg 8.944444e+02 train accuracy: 0.369735 val accuracy: 0.377000\n",
      "lr 3.000000e-07 reg 1.115556e+03 train accuracy: 0.376612 val accuracy: 0.361000\n",
      "lr 3.000000e-07 reg 1.336667e+03 train accuracy: 0.386633 val accuracy: 0.385000\n",
      "lr 3.000000e-07 reg 1.557778e+03 train accuracy: 0.385265 val accuracy: 0.384000\n",
      "lr 3.000000e-07 reg 1.778889e+03 train accuracy: 0.390224 val accuracy: 0.392000\n",
      "lr 3.000000e-07 reg 2.000000e+03 train accuracy: 0.386000 val accuracy: 0.391000\n",
      "lr 4.888889e-07 reg 1.000000e+01 train accuracy: 0.330816 val accuracy: 0.333000\n",
      "lr 4.888889e-07 reg 2.311111e+02 train accuracy: 0.356041 val accuracy: 0.346000\n",
      "lr 4.888889e-07 reg 4.522222e+02 train accuracy: 0.379898 val accuracy: 0.375000\n",
      "lr 4.888889e-07 reg 6.733333e+02 train accuracy: 0.392286 val accuracy: 0.389000\n",
      "lr 4.888889e-07 reg 8.944444e+02 train accuracy: 0.393755 val accuracy: 0.402000\n",
      "lr 4.888889e-07 reg 1.115556e+03 train accuracy: 0.394245 val accuracy: 0.399000\n",
      "lr 4.888889e-07 reg 1.336667e+03 train accuracy: 0.396490 val accuracy: 0.403000\n",
      "lr 4.888889e-07 reg 1.557778e+03 train accuracy: 0.397490 val accuracy: 0.387000\n",
      "lr 4.888889e-07 reg 1.778889e+03 train accuracy: 0.394469 val accuracy: 0.397000\n",
      "lr 4.888889e-07 reg 2.000000e+03 train accuracy: 0.393531 val accuracy: 0.405000\n",
      "lr 6.777778e-07 reg 1.000000e+01 train accuracy: 0.344980 val accuracy: 0.331000\n",
      "lr 6.777778e-07 reg 2.311111e+02 train accuracy: 0.381163 val accuracy: 0.397000\n",
      "lr 6.777778e-07 reg 4.522222e+02 train accuracy: 0.401102 val accuracy: 0.392000\n",
      "lr 6.777778e-07 reg 6.733333e+02 train accuracy: 0.404041 val accuracy: 0.401000\n",
      "lr 6.777778e-07 reg 8.944444e+02 train accuracy: 0.400245 val accuracy: 0.394000\n",
      "lr 6.777778e-07 reg 1.115556e+03 train accuracy: 0.391918 val accuracy: 0.396000\n",
      "lr 6.777778e-07 reg 1.336667e+03 train accuracy: 0.397469 val accuracy: 0.412000\n",
      "lr 6.777778e-07 reg 1.557778e+03 train accuracy: 0.399633 val accuracy: 0.406000\n",
      "lr 6.777778e-07 reg 1.778889e+03 train accuracy: 0.390592 val accuracy: 0.389000\n",
      "lr 6.777778e-07 reg 2.000000e+03 train accuracy: 0.390959 val accuracy: 0.396000\n",
      "lr 8.666667e-07 reg 1.000000e+01 train accuracy: 0.355673 val accuracy: 0.362000\n",
      "lr 8.666667e-07 reg 2.311111e+02 train accuracy: 0.395694 val accuracy: 0.382000\n",
      "lr 8.666667e-07 reg 4.522222e+02 train accuracy: 0.407224 val accuracy: 0.403000\n",
      "lr 8.666667e-07 reg 6.733333e+02 train accuracy: 0.406286 val accuracy: 0.413000\n",
      "lr 8.666667e-07 reg 8.944444e+02 train accuracy: 0.405143 val accuracy: 0.396000\n",
      "lr 8.666667e-07 reg 1.115556e+03 train accuracy: 0.402755 val accuracy: 0.397000\n",
      "lr 8.666667e-07 reg 1.336667e+03 train accuracy: 0.396347 val accuracy: 0.410000\n",
      "lr 8.666667e-07 reg 1.557778e+03 train accuracy: 0.390184 val accuracy: 0.388000\n",
      "lr 8.666667e-07 reg 1.778889e+03 train accuracy: 0.389367 val accuracy: 0.391000\n",
      "lr 8.666667e-07 reg 2.000000e+03 train accuracy: 0.384184 val accuracy: 0.396000\n",
      "lr 1.055556e-06 reg 1.000000e+01 train accuracy: 0.365510 val accuracy: 0.347000\n",
      "lr 1.055556e-06 reg 2.311111e+02 train accuracy: 0.402980 val accuracy: 0.409000\n",
      "lr 1.055556e-06 reg 4.522222e+02 train accuracy: 0.405143 val accuracy: 0.401000\n",
      "lr 1.055556e-06 reg 6.733333e+02 train accuracy: 0.405041 val accuracy: 0.401000\n",
      "lr 1.055556e-06 reg 8.944444e+02 train accuracy: 0.406776 val accuracy: 0.398000\n",
      "lr 1.055556e-06 reg 1.115556e+03 train accuracy: 0.399959 val accuracy: 0.403000\n",
      "lr 1.055556e-06 reg 1.336667e+03 train accuracy: 0.397163 val accuracy: 0.405000\n",
      "lr 1.055556e-06 reg 1.557778e+03 train accuracy: 0.391204 val accuracy: 0.390000\n",
      "lr 1.055556e-06 reg 1.778889e+03 train accuracy: 0.382306 val accuracy: 0.394000\n",
      "lr 1.055556e-06 reg 2.000000e+03 train accuracy: 0.391694 val accuracy: 0.403000\n",
      "lr 1.244444e-06 reg 1.000000e+01 train accuracy: 0.369878 val accuracy: 0.368000\n",
      "lr 1.244444e-06 reg 2.311111e+02 train accuracy: 0.406857 val accuracy: 0.401000\n",
      "lr 1.244444e-06 reg 4.522222e+02 train accuracy: 0.411265 val accuracy: 0.403000\n",
      "lr 1.244444e-06 reg 6.733333e+02 train accuracy: 0.407612 val accuracy: 0.396000\n",
      "lr 1.244444e-06 reg 8.944444e+02 train accuracy: 0.404204 val accuracy: 0.411000\n",
      "lr 1.244444e-06 reg 1.115556e+03 train accuracy: 0.397245 val accuracy: 0.389000\n",
      "lr 1.244444e-06 reg 1.336667e+03 train accuracy: 0.392898 val accuracy: 0.385000\n",
      "lr 1.244444e-06 reg 1.557778e+03 train accuracy: 0.384776 val accuracy: 0.388000\n",
      "lr 1.244444e-06 reg 1.778889e+03 train accuracy: 0.380245 val accuracy: 0.405000\n",
      "lr 1.244444e-06 reg 2.000000e+03 train accuracy: 0.380265 val accuracy: 0.389000\n",
      "lr 1.433333e-06 reg 1.000000e+01 train accuracy: 0.376224 val accuracy: 0.362000\n",
      "lr 1.433333e-06 reg 2.311111e+02 train accuracy: 0.408633 val accuracy: 0.381000\n",
      "lr 1.433333e-06 reg 4.522222e+02 train accuracy: 0.409327 val accuracy: 0.409000\n",
      "lr 1.433333e-06 reg 6.733333e+02 train accuracy: 0.406204 val accuracy: 0.407000\n",
      "lr 1.433333e-06 reg 8.944444e+02 train accuracy: 0.401224 val accuracy: 0.380000\n",
      "lr 1.433333e-06 reg 1.115556e+03 train accuracy: 0.388531 val accuracy: 0.392000\n",
      "lr 1.433333e-06 reg 1.336667e+03 train accuracy: 0.394714 val accuracy: 0.400000\n",
      "lr 1.433333e-06 reg 1.557778e+03 train accuracy: 0.394061 val accuracy: 0.393000\n",
      "lr 1.433333e-06 reg 1.778889e+03 train accuracy: 0.388878 val accuracy: 0.387000\n",
      "lr 1.433333e-06 reg 2.000000e+03 train accuracy: 0.381000 val accuracy: 0.379000\n",
      "lr 1.622222e-06 reg 1.000000e+01 train accuracy: 0.381776 val accuracy: 0.368000\n",
      "lr 1.622222e-06 reg 2.311111e+02 train accuracy: 0.415939 val accuracy: 0.405000\n",
      "lr 1.622222e-06 reg 4.522222e+02 train accuracy: 0.403878 val accuracy: 0.399000\n",
      "lr 1.622222e-06 reg 6.733333e+02 train accuracy: 0.405000 val accuracy: 0.406000\n",
      "lr 1.622222e-06 reg 8.944444e+02 train accuracy: 0.402102 val accuracy: 0.405000\n",
      "lr 1.622222e-06 reg 1.115556e+03 train accuracy: 0.394857 val accuracy: 0.403000\n",
      "lr 1.622222e-06 reg 1.336667e+03 train accuracy: 0.390490 val accuracy: 0.397000\n",
      "lr 1.622222e-06 reg 1.557778e+03 train accuracy: 0.387592 val accuracy: 0.407000\n",
      "lr 1.622222e-06 reg 1.778889e+03 train accuracy: 0.392408 val accuracy: 0.388000\n",
      "lr 1.622222e-06 reg 2.000000e+03 train accuracy: 0.388143 val accuracy: 0.393000\n",
      "lr 1.811111e-06 reg 1.000000e+01 train accuracy: 0.386429 val accuracy: 0.363000\n",
      "lr 1.811111e-06 reg 2.311111e+02 train accuracy: 0.414551 val accuracy: 0.408000\n",
      "lr 1.811111e-06 reg 4.522222e+02 train accuracy: 0.408918 val accuracy: 0.401000\n",
      "lr 1.811111e-06 reg 6.733333e+02 train accuracy: 0.398347 val accuracy: 0.407000\n",
      "lr 1.811111e-06 reg 8.944444e+02 train accuracy: 0.399490 val accuracy: 0.389000\n",
      "lr 1.811111e-06 reg 1.115556e+03 train accuracy: 0.389082 val accuracy: 0.387000\n",
      "lr 1.811111e-06 reg 1.336667e+03 train accuracy: 0.390959 val accuracy: 0.396000\n",
      "lr 1.811111e-06 reg 1.557778e+03 train accuracy: 0.388510 val accuracy: 0.406000\n",
      "lr 1.811111e-06 reg 1.778889e+03 train accuracy: 0.388122 val accuracy: 0.393000\n",
      "lr 1.811111e-06 reg 2.000000e+03 train accuracy: 0.382265 val accuracy: 0.375000\n",
      "lr 2.000000e-06 reg 1.000000e+01 train accuracy: 0.389184 val accuracy: 0.351000\n",
      "lr 2.000000e-06 reg 2.311111e+02 train accuracy: 0.414143 val accuracy: 0.410000\n",
      "lr 2.000000e-06 reg 4.522222e+02 train accuracy: 0.403551 val accuracy: 0.390000\n",
      "lr 2.000000e-06 reg 6.733333e+02 train accuracy: 0.398143 val accuracy: 0.391000\n",
      "lr 2.000000e-06 reg 8.944444e+02 train accuracy: 0.396837 val accuracy: 0.400000\n",
      "lr 2.000000e-06 reg 1.115556e+03 train accuracy: 0.389041 val accuracy: 0.376000\n",
      "lr 2.000000e-06 reg 1.336667e+03 train accuracy: 0.392082 val accuracy: 0.383000\n",
      "lr 2.000000e-06 reg 1.557778e+03 train accuracy: 0.380449 val accuracy: 0.386000\n",
      "lr 2.000000e-06 reg 1.778889e+03 train accuracy: 0.374980 val accuracy: 0.371000\n",
      "lr 2.000000e-06 reg 2.000000e+03 train accuracy: 0.382612 val accuracy: 0.385000\n",
      "best validation accuracy achieved during cross-validation: 0.413000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "##################################\n",
    "# rk's remark:                   #\n",
    "#          random search         #\n",
    "##################################\n",
    "\n",
    "# sample learning_rates\n",
    "exp_lr = -10 * np.random.rand(10)\n",
    "learning_rates = np.power(10, exp_lr)\n",
    "\n",
    "# sample regs\n",
    "exp_reg = 7 * np.random.rand(10)\n",
    "reg_strengths = np.power(10, exp_reg)\n",
    "para_group = [(np.random.choice(learning_rates, replace=False), \n",
    "               np.random.choice(reg_strengths, replace=False)) for _ in range(20)]\n",
    "\n",
    "###  comment out after try out coarse grid\n",
    "# for lr, reg in para_group:\n",
    "#     softmax = Softmax()\n",
    "#     loss_history = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "#                                  num_iters=2000, verbose=True)\n",
    "#     y_train_pred = softmax.predict(X_train)\n",
    "#     train_acc = np.sum((y_train_pred == y_train)) / y_train.shape[0]\n",
    "#     y_val_pred = softmax.predict(X_val)\n",
    "#     val_acc = np.sum((y_val_pred == y_val)) / y_val.shape[0]\n",
    "#     if(val_acc > best_val):\n",
    "#         best_val=val_acc\n",
    "#         best_softmax=softmax\n",
    "#     results[(lr,reg)]=(train_acc,val_acc)\n",
    "###  best result from coarse grid: \n",
    "###  => best validation accuracy achieved during cross-validation: 0.416000\n",
    "\n",
    "### refine grid\n",
    "lr_fine = [3e-7, 2e-6]\n",
    "reg_fine = [1e1, 2e3]\n",
    "\n",
    "for lr in np.linspace(lr_fine[0], lr_fine[1], 10):\n",
    "    for reg in np.linspace(reg_fine[0], reg_fine[1], 10):\n",
    "        softmax = Softmax()\n",
    "        loss_history = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                                     num_iters=2000, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_acc = np.sum((y_train_pred == y_train)) / y_train.shape[0]\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.sum((y_val_pred == y_val)) / y_val.shape[0]\n",
    "        if(val_acc > best_val):\n",
    "            best_val=val_acc\n",
    "            best_softmax=softmax\n",
    "        results[(lr,reg)]=(train_acc,val_acc)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1f9db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:54:06.000089Z",
     "start_time": "2024-07-24T13:54:05.977782Z"
    },
    "test": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.395000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9b65c",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009f08b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T13:54:10.553293Z",
     "start_time": "2024-07-24T13:54:10.084132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569b36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
